

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta property="og:title" content="Likelihood, greed and temperature in sequence learning" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://shaoweilin.github.io/posts/2022-05-28-likelihood-greed-and-temperature-in-sequence-learning/" />
<meta property="og:site_name" content="Types from Spikes" />
<meta property="og:description" content="Imagine we have a model D(w) of a dynamical system with states s \in S, that is parametrized by some weight w \in W. Each state s comes with a set N(s) \subset S of neighbors and an associated ener..." />
<meta property="og:image" content="https://shaoweilin.github.io/_static/profile.jpg" />
<meta property="og:image:alt" content="Types from Spikes" />
<meta name="description" content="Imagine we have a model D(w) of a dynamical system with states s \in S, that is parametrized by some weight w \in W. Each state s comes with a set N(s) \subset S of neighbors and an associated ener..." />

    <title>Likelihood, greed and temperature in sequence learning &#8212; Types from Spikes  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posts/2022-05-28-likelihood-greed-and-temperature-in-sequence-learning';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about/" />
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/> 
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../blog/atom.xml"
  title="Types from Spikes"
/>
 
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this site..."
         aria-label="Search this site..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../">
  
  
  
  
  
    <p class="title logo__title">Types from Spikes</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../about/">
                        About
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../blog/">
                        Blog
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this site..."
         aria-label="Search this site..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/shaoweilin/" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../about/">
                        About
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../blog/">
                        Blog
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this site..."
         aria-label="Search this site..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/shaoweilin/" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">  
<h2>
   <i class="fa fa-calendar"></i>
  28 May 2022 
</h2>

<ul>
        
</ul>
</div>
        <div class="sidebar-primary-item">
<h3>
  <a href="../../blog/">Recent Posts</a>
</h3>
<ul>
   
  <li>
    <a href="../2024-10-16-singular-learning-relative-information-and-the-dual-numbers/"
      >16 October - Singular learning, relative information and the dual numbers</a
    >
  </li>
  
  <li>
    <a href="../2024-09-24-safety-by-shared-synthesis/"
      >24 September - Safety by shared synthesis</a
    >
  </li>
  
  <li>
    <a href="../2024-05-22-formal-ai-assisted-code-specification-and-synthesis-concrete-steps-towards-safe-sociotechnical-systems/"
      >22 May - Formal AI-assisted code specification and synthesis: concrete steps towards safe sociotechnical systems</a
    >
  </li>
  
  <li>
    <a href="../2024-05-08-ai-assisted-coding-correct-by-construction-not-by-generation/"
      >08 May - AI-assisted coding: correct by construction, not by generation</a
    >
  </li>
  
  <li>
    <a href="../2024-03-21-prior-work-on-program-synthesis/"
      >21 March - Program Synthesis</a
    >
  </li>
  
</ul>
</div>
        <div class="sidebar-primary-item">
<h3>
  <a href="../../blog/archive/">Archives</a>
</h3>
<ul>
   
  <li>
    <a href="../../blog/2024/">2024 (5)</a>
  </li>
    
  <li>
    <a href="../../blog/2023/">2023 (5)</a>
  </li>
    
  <li>
    <a href="../../blog/2022/">2022 (3)</a>
  </li>
    
  <li>
    <a href="../../blog/2021/">2021 (8)</a>
  </li>
    
  <li>
    <a href="../../blog/2020/">2020 (12)</a>
  </li>
    
  <li>
    <a href="../../blog/2018/">2018 (1)</a>
  </li>
    
  <li>
    <a href="../../blog/2017/">2017 (1)</a>
  </li>
    
  <li>
    <a href="../../blog/2016/">2016 (3)</a>
  </li>
    
  <li>
    <a href="../../blog/2014/">2014 (2)</a>
  </li>
    
  <li>
    <a href="../../blog/2012/">2012 (1)</a>
  </li>
   
</ul>
</div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Likelihood, greed and temperature in sequence learning</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                   <div class="tex2jax_ignore mathjax_ignore section" id="likelihood-greed-and-temperature-in-sequence-learning">
<h1>Likelihood, greed and temperature in sequence learning<a class="headerlink" href="#likelihood-greed-and-temperature-in-sequence-learning" title="Permalink to this headline">#</a></h1>
<p>Imagine we have a model <span class="math notranslate nohighlight">\(D(w)\)</span> of a dynamical system with states <span class="math notranslate nohighlight">\(s \in S,\)</span> that is parametrized by some weight <span class="math notranslate nohighlight">\(w \in W\)</span>. Each state <span class="math notranslate nohighlight">\(s\)</span> comes with a set <span class="math notranslate nohighlight">\(N(s) \subset S\)</span> of neighbors and an associated energy function <span class="math notranslate nohighlight">\(E(s'|s,w) \in \mathbb{R}\)</span> that assigns an energy to each neighbor <span class="math notranslate nohighlight">\(s' \in N(s)\)</span>.</p>
<p>For simplicity, we assume the following dynamics: when the system is in state <span class="math notranslate nohighlight">\(s\)</span>, it picks the neighbor <span class="math notranslate nohighlight">\(s'\)</span> with the lowest energy <span class="math notranslate nohighlight">\(E(s'|s,w)\)</span> and jumps to state <span class="math notranslate nohighlight">\(s'\)</span> in the next time step (more to come later about what we mean by <em>time step</em>).</p>
<p>Suppose also that we have an observed sequence <span class="math notranslate nohighlight">\(\{\hat{s}_t\}\)</span> with each <span class="math notranslate nohighlight">\(t\in \{0, 1, \ldots, T\}.\)</span> Roughly, the goal is to find a dynamical system <span class="math notranslate nohighlight">\(D(\hat{w})\)</span> that is able to reproduce <span class="math notranslate nohighlight">\(\hat{s}_t\)</span>. This is the <em>sequence learning</em> problem for the model <span class="math notranslate nohighlight">\(D(w)\)</span>.</p>
<p>More specifically, we want to answer the following questions.</p>
<ol class="arabic simple">
<li><p>Is there a likelihood-based method for sequence learning in discrete time?</p></li>
<li><p>Is there a likelihood-based method for sequence learning in continuous time?</p></li>
<li><p>Is there a simple and efficient greedy method for sequence learning?</p></li>
<li><p>How is the greedy method related to the likelihood method?</p></li>
<li><p>Are there other dynamical systems more suited for natural language processing or reinforcement learning?</p></li>
</ol>
<div class="section" id="likelihood-method-in-discrete-time">
<h2>Likelihood method in discrete time<a class="headerlink" href="#likelihood-method-in-discrete-time" title="Permalink to this headline">#</a></h2>
<p>Given the (conditional) energy function <span class="math notranslate nohighlight">\(E(s'|s,w),\)</span> we consider a discrete-time Markov chain with transition probabilities</p>
<div class="math notranslate nohighlight">
\[
P(s'|s, w) = \displaystyle \frac{\exp(-\beta E(s'|s,w)) }{Z(s,w)} 
\]</div>
<p>where we have the <em>partition function</em></p>
<div class="math notranslate nohighlight">
\[
Z(s,w) = \displaystyle \sum_{s' \in N(s)} \exp(-\beta E(s'|s,w))
\]</div>
<p>and the <em>inverse temperature</em> <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>The traditional maximum likelihood method for sequence learning tells us to maximize the product</p>
<div class="math notranslate nohighlight">
\[
L(w) = \prod_{t=0}^{T-1} P(\hat{s}_{t+1}|\hat{s}_t, w).
\]</div>
<p>The negative log-likelihood is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(w) &amp;= \displaystyle \sum_{t=0}^{T-1} \log Z(\hat{s}_t,w)+\beta \sum_{t=0}^{T-1} E(\hat{s}_{t+1}|\hat{s}_t, w) \\ 
&amp;= \displaystyle \sum_{t=0}^{T-1} \log \sum_{s' \in N(\hat{s}_t)} e^{-\beta E(s'|\hat{s}_t,w)}+\beta \sum_{t=0}^{T-1} E(\hat{s}_{t+1}|\hat{s}_t, w) 
\end{align*}\]</div>
<p>The tricky issue here is the intractable logarithmic partition function <span class="math notranslate nohighlight">\(\log Z(s,w)\)</span> when we have a large number of neighboring states.</p>
</div>
<div class="section" id="likelihood-method-in-continuous-time">
<h2>Likelihood method in continuous time<a class="headerlink" href="#likelihood-method-in-continuous-time" title="Permalink to this headline">#</a></h2>
<p>To overcome the intractable partition function, we could consider a continuous-time Markov chain with off-diagonal transition rates</p>
<div class="math notranslate nohighlight">
\[
\Gamma(s'|s,w) = \exp(-\beta E(s'|s,w))
\]</div>
<p>and diagonal transition rates</p>
<div class="math notranslate nohighlight">
\[ 
\Gamma(s|s,w) = - \sum_{s'\in N(s)} \Gamma(s'|s,w) = - Z(s,w)
\]</div>
<p>If we assume a small time interval <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> and construct a continuous time series <span class="math notranslate nohighlight">\(\hat{x}(t)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\hat{x}_{\delta t} =  \hat{s}_{\lfloor t\rfloor} ,
\quad \text{for all }0 \leq t \leq T,
\]</div>
<p>then the likelihood of <span class="math notranslate nohighlight">\(\hat{x}\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L(w) &amp; =\displaystyle \prod_{t=0}^{N-1} 
Z(\hat{s}_{t},w) \exp \big(-\delta Z(\hat{s}_t,w) \big) \, \displaystyle \frac{\exp(-\beta E(s'|s,w))}{Z(\hat{s}_t,w)} \\ 
&amp; =\displaystyle \prod_{t=0}^{N-1} 
  \exp \big(-\delta Z(\hat{s}_t,w) \big) \, \exp(-\beta E(s'|s,w)).
\end{align*}\]</div>
<p>The negative log-likelihood is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(w) &amp;= \displaystyle\delta \sum_{t=0}^{T-1} Z(\hat{s}_t,w) +\beta \sum_{t=0}^{T-1} E(\hat{s}_{t+1}|\hat{s}_t, w)\\ &amp;= \displaystyle\delta\sum_{t=0}^{T-1} \sum_{s' \in N(\hat{s}_t)} e^{-\beta E(s'|\hat{s}_t,w)} +\beta \sum_{t=0}^{T-1} E(\hat{s}_{t+1}|\hat{s}_t, w).
\end{align*}\]</div>
<p>Again, the intractable partition function appears but without a logarithm, so it is easier to differentiate under the sum over the neighboring states. We can reduce its contribution by either making <span class="math notranslate nohighlight">\(\beta\)</span> large or making <span class="math notranslate nohighlight">\(\delta\)</span> small. Note that we cannot make the hyperparameter <span class="math notranslate nohighlight">\(\delta\)</span> go away  in the objective function just by scaling <span class="math notranslate nohighlight">\(\beta.\)</span></p>
</div>
<div class="section" id="relative-information">
<h2>Relative information<a class="headerlink" href="#relative-information" title="Permalink to this headline">#</a></h2>
<p>The negative log-likelihoods for both the discrete-time and continuous-time models can be interpreted in terms of (conditional) relative information. See <a class="reference external" href="https://shaoweilin.github.io/posts/2020-10-23-machine-learning-with-relative-information/">this post</a> for more details. These <em>energy-based</em> methods can be thought of as <em>information-based</em> methods.</p>
</div>
<div class="section" id="greedy-method">
<h2>Greedy method<a class="headerlink" href="#greedy-method" title="Permalink to this headline">#</a></h2>
<p>In the naive greedy method, we simply look for a weight <span class="math notranslate nohighlight">\(\hat{w}\)</span> such that the following inequalities are satisfied for all <span class="math notranslate nohighlight">\(t\in \{0, \ldots, T-1\}\)</span> and <span class="math notranslate nohighlight">\(s' \in N(\hat{s}_t), s' \neq \hat{s}_{t+1}:\)</span></p>
<div class="math notranslate nohighlight">
\[
E(s'|\hat{s}_t, w) \leq E(\hat{s}_{t-1}|\hat{s}_t, w).
\]</div>
<p>If such a weight <span class="math notranslate nohighlight">\(\hat{w}\)</span> exists, then the dynamical system <span class="math notranslate nohighlight">\(D(\hat{w})\)</span> will faithfully reproduce the sequence <span class="math notranslate nohighlight">\(\{\hat{s}_t\}.\)</span> We say that <span class="math notranslate nohighlight">\(\hat{w}\)</span> is <em>faithful</em> to <span class="math notranslate nohighlight">\(\{\hat{s}_t\}\)</span>.</p>
<p>If such a weight <span class="math notranslate nohighlight">\(\hat{w}\)</span> is not faithful to <span class="math notranslate nohighlight">\(\{\hat{s}_t\}\)</span>, then we need to fall back to the likelihood methods to find a dynamical system <span class="math notranslate nohighlight">\(D(\hat{w})\)</span> that best approximates the observed sequence.</p>
<p>How is the greedy method related to the likelihood methods?</p>
<p>Given a weight <span class="math notranslate nohighlight">\(w \in W\)</span> and state <span class="math notranslate nohighlight">\(s_t \in S\)</span> at time <span class="math notranslate nohighlight">\(t,\)</span> let <span class="math notranslate nohighlight">\(s^*_{t+1} \in N(s_t)\)</span> denote a neighboring state at time <span class="math notranslate nohighlight">\(t+1\)</span> that minimizes the energy <span class="math notranslate nohighlight">\(E(s^*_{t+1}|s_t,w).\)</span> We consider what happens in the discrete-time and continuous-time models as the inverse temperature <span class="math notranslate nohighlight">\(\beta\)</span> tends to infinity.</p>
</div>
<div class="section" id="temperature-in-discrete-time">
<h2>Temperature in discrete time<a class="headerlink" href="#temperature-in-discrete-time" title="Permalink to this headline">#</a></h2>
<p>In the discrete-time model, the transition probability <span class="math notranslate nohighlight">\(P(s^*_{t+1}|s_t, w)\)</span> will tend to <span class="math notranslate nohighlight">\(1,\)</span> while the transition probabilities of other neighbors of <span class="math notranslate nohighlight">\(s_t\)</span> will tend to <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(w\)</span> is faithful the the observed sequence <span class="math notranslate nohighlight">\(\{\hat{s}_t\},\)</span> then the likelihood <span class="math notranslate nohighlight">\(L(w)\)</span> will tend to <span class="math notranslate nohighlight">\(1\)</span> as <span class="math notranslate nohighlight">\(\beta\)</span> goes to infinity. If <span class="math notranslate nohighlight">\(w\)</span> is not faithful, then <span class="math notranslate nohighlight">\(L(w)\)</span> will tend to <span class="math notranslate nohighlight">\(0.\)</span> If the region of faithful weights is non-empty, then as the maximum likelihood estimate <span class="math notranslate nohighlight">\(\hat{w}\)</span> will lie in this region for sufficiently large <span class="math notranslate nohighlight">\(\beta.\)</span></p>
<p>The log partition function</p>
<div class="math notranslate nohighlight">
\[
\log Z(s_t, w) = \log \sum_{s'\in N(s_t)}  e^{-\beta E(s'|s_t,w)}
\]</div>
<p>tends to <span class="math notranslate nohighlight">\(-\beta E(s^*_{t+1}|s_t,w)\)</span> as <span class="math notranslate nohighlight">\(\beta\)</span> goes to infinity (assuming there is a unique minimal energy state <span class="math notranslate nohighlight">\(\hat{s}^*_{t+1}.\)</span>) Consequently, the negative log-likelihood <span class="math notranslate nohighlight">\(\ell(w)\)</span> tends to</p>
<div class="math notranslate nohighlight">
\[
\beta \sum_{t=0}^{T-1} \Big( E(\hat{s}_{t+1}|\hat{s}_t, w) - E(\hat{s}^*_{t+1}|\hat{s}_t, w) \Big),
\]</div>
<p>which is <span class="math notranslate nohighlight">\(0\)</span> if <span class="math notranslate nohighlight">\(w\)</span> is faithful to the sequence but positive if <span class="math notranslate nohighlight">\(w\)</span> is not faithful.</p>
<p>In the large <span class="math notranslate nohighlight">\(\beta\)</span> limit, we generate an optimal sequence or path by running the dynamical system described at the beginning of this post - namely, at each state <span class="math notranslate nohighlight">\(s\)</span>, we pick the next state <span class="math notranslate nohighlight">\(s'\)</span> that minimizes the energy <span class="math notranslate nohighlight">\(E(s'|s,w),\)</span> and we repeat this for <span class="math notranslate nohighlight">\(T\)</span> time steps. We say that the optimal path is generated by <em>stepwise energy minimization</em>.</p>
</div>
<div class="section" id="temperature-in-continuous-time">
<h2>Temperature in continuous time<a class="headerlink" href="#temperature-in-continuous-time" title="Permalink to this headline">#</a></h2>
<p>In the continuous-time model, as <span class="math notranslate nohighlight">\(\beta\)</span> goes to infinity, the transition probability <span class="math notranslate nohighlight">\(P(s^*_{t+1}|s_t, w)\)</span> tends to <span class="math notranslate nohighlight">\(1\)</span> while the transition probability of other neighbors <span class="math notranslate nohighlight">\(s'_{t+1}\)</span> given <span class="math notranslate nohighlight">\(s_t\)</span> tends to <span class="math notranslate nohighlight">\(0,\)</span> just like in the discrete-time case. As seen previously, the negative log probability grows like <span class="math notranslate nohighlight">\(\beta \big( E(\hat{s}_{t+1}|\hat{s}_t, w) - E(\hat{s}^*_{t+1}|\hat{s}_t, w) \big) \)</span> for large <span class="math notranslate nohighlight">\(\beta.\)</span></p>
<p>The holding rates <span class="math notranslate nohighlight">\(Z(s_t, w)\)</span> will, however, tend to <span class="math notranslate nohighlight">\(0.\)</span> This means that the time held in state <span class="math notranslate nohighlight">\(s_t\)</span> will increase to infinity as <span class="math notranslate nohighlight">\(\beta\)</span> increases.</p>
<p>In fact, for large <span class="math notranslate nohighlight">\(\beta,\)</span> the negative log density</p>
<div class="math notranslate nohighlight">
\[
-\log \left[ Z(\hat{s}_{t},w) \exp \big(-\delta Z(\hat{s}_t,w) \big) \right] = \delta Z(\hat{s}_t,w) -\log Z(\hat{s}_{t},w) 
\]</div>
<p>of exponentially-distributed holding time grows like <span class="math notranslate nohighlight">\(\beta E(\hat{s}^*_{t+1}|\hat{s}_t, w),\)</span> the transition energy that is mimimal over all <span class="math notranslate nohighlight">\(\beta E(s'|\hat{s}_t, w)\)</span> for <span class="math notranslate nohighlight">\(s' \in N(\hat{s}_t).\)</span> This means that for lower minimal transition energies <span class="math notranslate nohighlight">\(E(\hat{s}^*_{t+1}|\hat{s}_t, w),\)</span> the holding time period in state <span class="math notranslate nohighlight">\(\hat{s}_t\)</span> is shorter.</p>
<p>Consequently, the negative log-likelihood <span class="math notranslate nohighlight">\(\ell(w)\)</span> tends to</p>
<div class="math notranslate nohighlight">
\[
\beta \sum_{t=0}^{T-1} E(\hat{s}_{t+1}|\hat{s}_t, w),
\]</div>
<p>the sum of the transition energies. The objective here is different from that in the discrete-time model which
subtracts the minimal transition energy from each time step.</p>
<p>Suppose that we have a time interval of length <span class="math notranslate nohighlight">\(T\)</span> and the initial state is <span class="math notranslate nohighlight">\(s_0\)</span>. By the stochastic summation algorithm of Gillespie <span id="id1">[<a class="reference internal" href="#id13" title="Markus F Weber and Erwin Frey. Master equations and the theory of stochastic path integrals. Reports on Progress in Physics, 80(4):046601, 2017.">WF17</a>]</span>, the negative log density of observing a path with states <span class="math notranslate nohighlight">\(s_0, \ldots, s_n\)</span> and holding periods <span class="math notranslate nohighlight">\(\delta_0, \ldots, \delta_n\)</span> satisfying <span class="math notranslate nohighlight">\(\delta_0 + \cdots +\delta_n = T,\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\sum_{t=0}^n \delta_t Z(s_t,w) + \beta \sum_{t=0}^{n-1} E(s_{t+1}|s_t,w).
\]</div>
<p>For large <span class="math notranslate nohighlight">\(\beta\)</span>, the latter summand dominates the log density, and the optimal paths are determined by <em>pathwise energy minimization</em>. This pathwise minimization is analogous to the principle of least action in classical or quantum physics.</p>
</div>
<div class="section" id="stepwise-vs-pathwise-minimization">
<h2>Stepwise vs pathwise minimization<a class="headerlink" href="#stepwise-vs-pathwise-minimization" title="Permalink to this headline">#</a></h2>
<p>The following Markov chain demonstrates the difference between stepwise and pathwise energy minimization. It has four states <span class="math notranslate nohighlight">\(A,B,C,D\)</span> and an arrow between states from state <span class="math notranslate nohighlight">\(i\)</span> to state <span class="math notranslate nohighlight">\(j\)</span> with weight <span class="math notranslate nohighlight">\(w\)</span> indicates that the energy is <span class="math notranslate nohighlight">\(E(j|i) = w\)</span>.</p>
<div align="center" class="align-center"><div class="graphviz"><object data="../../_images/graphviz-ee1a898a0ea60906b3855635a445005d6465cc57.svg" type="image/svg+xml" class="graphviz">
<p class="warning">digraph markov_chain {
	fontname=&quot;Helvetica,Arial,sans-serif&quot;
	node [fontname=&quot;Helvetica,Arial,sans-serif&quot;]
	edge [fontname=&quot;Helvetica,Arial,sans-serif&quot;]
	rankdir=LR;
	node [shape = circle];
	A -&gt; B [label = &quot;1&quot;];
	A -&gt; C [label = &quot;2&quot;];
	B -&gt; D [label = &quot;100&quot;];
	C -&gt; D [label = &quot;1&quot;];
}</p></object></div>
</div>
<p>Under stepwise minimization, the transition <span class="math notranslate nohighlight">\(A \rightarrow B\)</span> is chosen because it has a lower energy than <span class="math notranslate nohighlight">\(A \rightarrow C\)</span>. There is only one transition <span class="math notranslate nohighlight">\(B \rightarrow D\)</span> so that is automatically picked. The total path energy is <span class="math notranslate nohighlight">\(101\)</span>.</p>
<p>Under pathwise minimization, there are two paths: <span class="math notranslate nohighlight">\(A \rightarrow B \rightarrow D\)</span> with energy <span class="math notranslate nohighlight">\(101\)</span>, and <span class="math notranslate nohighlight">\(A \rightarrow C \rightarrow D\)</span> with energy <span class="math notranslate nohighlight">\(3\)</span>. Therefore, the chosen path will be <span class="math notranslate nohighlight">\(A \rightarrow C \rightarrow D\)</span>.</p>
<p>Given a natural number <span class="math notranslate nohighlight">\(n &gt; 0\)</span> and state <span class="math notranslate nohighlight">\(x,\)</span> let <span class="math notranslate nohighlight">\(\mathcal{P}(x,n)\)</span> denote the set of paths with minimum path energies among all paths of length <span class="math notranslate nohighlight">\(n\)</span> starting at state <span class="math notranslate nohighlight">\(x,\)</span> and let <span class="math notranslate nohighlight">\(\mathcal{E}(x, n)\)</span> be the minimum energy attained. Here, the length of a path is the number of edges in the path. We can recursively compute them by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{E}(x,0) &amp;= 0\\
\mathcal{E}(x,n+1) &amp;= \min_{y \in N(x)} \left( E(y|x) + \mathcal{E}(y, n) \right)
\end{align*}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{P}(x,0) &amp;= \{x\}\\
\mathcal{P}(x,n+1) &amp;= \{x \rightarrow p \mid y \in S, p \in \mathcal{P}(y, n), \\
&amp; \qquad \mathcal{E}(x,n+1) = E(y|x)+\mathcal{E}(y,n) \}
\end{align*}
\end{split}\]</div>
<p>for all states <span class="math notranslate nohighlight">\(x\)</span> and lengths <span class="math notranslate nohighlight">\(n,\)</span> where <span class="math notranslate nohighlight">\(x \rightarrow p\)</span> denotes the prepending of a state <span class="math notranslate nohighlight">\(x\)</span> to a path <span class="math notranslate nohighlight">\(p\)</span>.</p>
</div>
<div class="section" id="natural-language-processing">
<h2>Natural language processing<a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">#</a></h2>
<p>The difference between stepwise energy minimization and pathwise energy minimization is easily seen in natural language processing.</p>
<p>In his blog <span id="id2">[<a class="reference internal" href="#id14" title="Andrej Karpathy Blog. The unreasonable effectiveness of recurrent neural networks. URL: http://karpathy. github. io/2015/05/21/rnn-effectiveness/dated May, 21:33, 2015.">Blo15</a>]</span> about the unreasonable effectiveness of recurrent neural networks (RNN), Karpathy trained a small character-level LSTM (long short-term memory) network on the writings of Paul Graham.</p>
<p>He found that for large inverse temperatures <span class="math notranslate nohighlight">\(\beta,\)</span> the network generated the following stepwise greedy sequence of words:</p>
<blockquote>
<div><p>is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same…</p>
</div></blockquote>
<p>Granted that at lower inverse temperatures the network produced sentences with greater variation and that the network is technically not a Markov chain, the above example shows us what could go wrong with extreme stepwise greedy sentence generation.</p>
<p>Perhaps our minds behave more like networks that churn out sentences with small pathwise energies.</p>
<p>Why would nature choose such a strategy over the simpler stepwise greedy approach, in natural language processing and, more generally, in reinforcement learning?</p>
</div>
<div class="section" id="reinforcement-learning">
<h2>Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">#</a></h2>
<p>One possible answer is that many natural problems are best solved by <em>trial and error</em>. One has to try many <em>likely</em> solutions in a short period of time, as opposed to getting the correct solution but only after a long deliberation.</p>
<p>In a continuous-time Markov chain, a path with optimal stepwise energy may suffer from large minimal transition energies <span class="math notranslate nohighlight">\(E(\hat{s}^*_{t+1}|\hat{s}_t, w)\)</span> which lead to long holding times.</p>
<p>On the other hand, a path with optimal pathwise energy may have non-optimal transitions, but the system can generate many such paths quickly because of their short holding times. One of these paths could be the key to the problem at hand.</p>
<p>Even in problems where small mistakes are catastrophic, the ability to brainstorm and quickly generate many possible solutions is critical. We can reason about the correctness of the solutions before testing them out. We can choose the best potential solution with the smallest risk.</p>
</div>
<div class="section" id="minimum-energy-flow">
<h2>Minimum energy flow<a class="headerlink" href="#minimum-energy-flow" title="Permalink to this headline">#</a></h2>
<p>We end with an interesting connection between the continuous-time likelihood method for sequence learning and the minimum energy flow (MEF) method for training Hopfield networks efficiently <span id="id3">[<a class="reference internal" href="#id15" title="Christopher Hillar, Tenzin Chan, Rachel Taubman, and David Rolnick. Hidden hypergraphs, error-correcting codes, and critical learning in hopfield networks. Entropy, 23(11):1494, 2021.">HCTR21</a>]</span>.</p>
<p>Recall that the negative log likelihood is</p>
<div class="math notranslate nohighlight">
\[
\ell(w) = \displaystyle\delta\sum_{t=0}^{T-1} \sum_{s' \in N(\hat{s}_t)} e^{-\beta E(s'|\hat{s}_t,w)} +\beta \sum_{t=0}^{T-1} E(\hat{s}_{t+1}|\hat{s}_t, w).
\]</div>
<p>On the other hand, the energy flow function is</p>
<div class="math notranslate nohighlight">
\[
\text{EF}(w) = \displaystyle\sum_{t=0}^{T-1} \sum_{s' \in N(\hat{s}_t)} e^{- E(s'|\hat{s}_t,w)},
\]</div>
<p>which is proportional to the limit of <span class="math notranslate nohighlight">\(\ell(w)\)</span> at <span class="math notranslate nohighlight">\(\beta =1\)</span> as <span class="math notranslate nohighlight">\(\delta\)</span> tends to infinity.</p>
<p>This limit makes sense because energy flow is used in the paper for training Hopfield networks with attractor dynamics. The observed data <span class="math notranslate nohighlight">\(\{\hat{s}_t\}\)</span> is not actually a sequence, but a list of attractors. Each  attractor may be thought of as a one-state sequence with infinite holding time <span class="math notranslate nohighlight">\(\delta\)</span>.</p>
<p>The negative log likelihood <span class="math notranslate nohighlight">\(\ell(w)\)</span> provides a family of objective functions with hyperparameters <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> that could be used in sequence learning.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id4">
<dl class="citation">
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id2">Blo15</a></span></dt>
<dd><p>Andrej Karpathy Blog. The unreasonable effectiveness of recurrent neural networks. <em>URL: http://karpathy. github. io/2015/05/21/rnn-effectiveness/dated May</em>, 21:33, 2015.</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id3">HCTR21</a></span></dt>
<dd><p>Christopher Hillar, Tenzin Chan, Rachel Taubman, and David Rolnick. Hidden hypergraphs, error-correcting codes, and critical learning in hopfield networks. <em>Entropy</em>, 23(11):1494, 2021.</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id1">WF17</a></span></dt>
<dd><p>Markus F Weber and Erwin Frey. Master equations and the theory of stochastic path integrals. <em>Reports on Progress in Physics</em>, 80(4):046601, 2017.</p>
</dd>
</dl>
</div>
</div>
</div>

<div class="section">
    

<div class="section">
  <span style="float: left">
     
    <a href="../2022-05-08-parametric-typeclasses-aid-generalization-in-program-synthesis/">
      <i class="fa fa-arrow-circle-left"></i> Parametric typeclasses aid generalization in program synthesis
    </a>
    
  </span>
  <span>&nbsp;</span>
  <span style="float: right">
     
    <a href="../2023-04-01-relative-information-is-motivic/">
      Relative information is motivic <i
        class="fa fa-arrow-circle-right"
      ></i
      >
    </a>
    
  </span>
</div>
  
</div>

                </article>
              
<p style="margin-bottom:5em;"></p>
<!-- Add a comment box underneath the page's content -->
<script src="https://giscus.app/client.js"
        data-repo="shaoweilin/shaoweilin.github.io"
        data-repo-id="R_kgDOHM5tAA"
        data-category="Blog comments"
        data-category-id="DIC_kwDOHM5tAM4COqgI"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-method-in-discrete-time">Likelihood method in discrete time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-method-in-continuous-time">Likelihood method in continuous time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-information">Relative information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-method">Greedy method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature-in-discrete-time">Temperature in discrete time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature-in-continuous-time">Temperature in continuous time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stepwise-vs-pathwise-minimization">Stepwise vs pathwise minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-language-processing">Natural language processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">Reinforcement learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum-energy-flow">Minimum energy flow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/posts/2022-05-28-likelihood-greed-and-temperature-in-sequence-learning.md.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2022, Shaowei Lin.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>