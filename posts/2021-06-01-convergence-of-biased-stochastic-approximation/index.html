

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta property="og:title" content="Convergence of biased stochastic approximation" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://shaoweilin.github.io/posts/2021-06-01-convergence-of-biased-stochastic-approximation/" />
<meta property="og:site_name" content="Types from Spikes" />
<meta property="og:description" content="Using techniques from biased stochastic approximation[ KMMW19], we prove under some regularity conditions the convergence of the online learning algorithm proposed previously for mutable Markov pro..." />
<meta property="og:image" content="https://shaoweilin.github.io/_static/profile.jpg" />
<meta property="og:image:alt" content="Types from Spikes" />
<meta name="description" content="Using techniques from biased stochastic approximation[ KMMW19], we prove under some regularity conditions the convergence of the online learning algorithm proposed previously for mutable Markov pro..." />

    <title>Convergence of biased stochastic approximation &#8212; Types from Spikes  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posts/2021-06-01-convergence-of-biased-stochastic-approximation';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about/" />
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/> 
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../blog/atom.xml"
  title="Types from Spikes"
/>
 
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this site..."
         aria-label="Search this site..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../">
  
  
  
  
  
    <p class="title logo__title">Types from Spikes</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../about/">
                        About
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../blog/">
                        Blog
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this site..."
         aria-label="Search this site..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/shaoweilin/" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../about/">
                        About
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../blog/">
                        Blog
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this site..."
         aria-label="Search this site..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/shaoweilin/" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">  
<h2>
   <i class="fa fa-calendar"></i>
  01 June 2021 
</h2>

<ul>
        
</ul>
</div>
        <div class="sidebar-primary-item">
<h3>
  <a href="../../blog/">Recent Posts</a>
</h3>
<ul>
   
  <li>
    <a href="../2023-08-20-references-on-information-cohomology/"
      >20 August - References on information cohomology</a
    >
  </li>
  
  <li>
    <a href="../2023-06-26-all-you-need-is-relative-information/"
      >26 June - All you need is relative information</a
    >
  </li>
  
  <li>
    <a href="../2023-04-01-relative-information-is-motivic/"
      >01 April - Relative information is motivic</a
    >
  </li>
  
  <li>
    <a href="../2022-05-28-likelihood-greed-and-temperature-in-sequence-learning/"
      >28 May - Likelihood, greed and temperature in sequence learning</a
    >
  </li>
  
  <li>
    <a href="../2022-05-08-parametric-typeclasses-aid-generalization-in-program-synthesis/"
      >22 January - Parametric typeclasses aid generalization in program synthesis</a
    >
  </li>
  
</ul>
</div>
        <div class="sidebar-primary-item">
<h3>
  <a href="../../blog/archive/">Archives</a>
</h3>
<ul>
   
  <li>
    <a href="../../blog/2023/">2023 (3)</a>
  </li>
    
  <li>
    <a href="../../blog/2022/">2022 (3)</a>
  </li>
    
  <li>
    <a href="../../blog/2021/">2021 (8)</a>
  </li>
    
  <li>
    <a href="../../blog/2020/">2020 (12)</a>
  </li>
    
  <li>
    <a href="../../blog/2018/">2018 (1)</a>
  </li>
    
  <li>
    <a href="../../blog/2017/">2017 (1)</a>
  </li>
    
  <li>
    <a href="../../blog/2016/">2016 (3)</a>
  </li>
    
  <li>
    <a href="../../blog/2014/">2014 (2)</a>
  </li>
    
  <li>
    <a href="../../blog/2012/">2012 (1)</a>
  </li>
   
</ul>
</div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Convergence of biased stochastic approximation</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                   <div class="tex2jax_ignore mathjax_ignore section" id="convergence-of-biased-stochastic-approximation">
<h1>Convergence of biased stochastic approximation<a class="headerlink" href="#convergence-of-biased-stochastic-approximation" title="Permalink to this headline">#</a></h1>
<p>Using techniques from <a class="reference internal" href="../2020-12-01-biased-stochastic-approximation/"><span class="doc std std-doc">biased</span></a> stochastic approximation <span id="id1">[<a class="reference internal" href="../2021-06-05-spiking-neural-networks/#id50" title="Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, 1944–1974. PMLR, 2019.">KMMW19</a>]</span>, we prove under some regularity conditions the convergence of the online learning algorithm proposed <a class="reference internal" href="../2021-03-23-biased-stochastic-approximation-with-mutable-processes/"><span class="doc std std-doc">previously</span></a> for mutable Markov processes.</p>
<p>Recall that the algorithm is described by the following updates.</p>
<div class="math notranslate nohighlight">
\[ \displaystyle 
X_{n+1} \sim Q_*(X_{n+1} \vert X_{n})\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
Z_{n+1} \sim Q_{\lambda_{n}}(Z_{n+1} \vert Z_{n}, X_{n})\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\theta_{n+1} = \theta_{n} + \eta_{n+1} \left.\frac{d}{d\theta} \log P_\theta(Z_{n+1}, X_{n+1} \vert Z_{n},X_{n}) \right\vert _{\theta = \theta_{n}}\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\alpha_{n+1} = \alpha_{n} + \left.\frac{d}{d\lambda} \log Q_{\lambda}(Z_{n+1} \vert  Z_{n},X_{n})\right\vert _{\lambda=\lambda_{n}}\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\gamma_{n+1} = \xi(X_{n+1} \vert X_n) + \log \frac{Q_{\lambda_{n}}(Z_{n+1}\vert Z_{n},X_{n})}{P_{\theta_{n}}(Z_{n+1},X_{n+1}\vert Z_{n},X_{n})} \]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\lambda_{n+1} = \lambda_{n} - \eta_{n+1} \alpha_{n+1} \gamma_{n+1}\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi(X_{n+1} \vert X_n)\)</span> is a function of the environment <span class="math notranslate nohighlight">\(X_{n+1}, X_n\)</span> that is independent of the parameters <span class="math notranslate nohighlight">\(\theta,\lambda\)</span> and the beliefs <span class="math notranslate nohighlight">\(Z_{n+1},Z_n.\)</span></p>
<p>This post is a continuation from our <a class="reference internal" href="../2020-08-28-motivic-information-path-integrals-and-spiking-networks/"><span class="doc std std-doc">series</span></a> on spiking networks, path integrals and motivic information.</p>
<div class="section" id="how-do-we-frame-the-problem-in-the-language-of-karimi2019non">
<h2>How do we frame the problem in the language of <span id="id2">[<a class="reference internal" href="../2021-06-05-spiking-neural-networks/#id50" title="Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, 1944–1974. PMLR, 2019.">KMMW19</a>]</span>?<a class="headerlink" href="#how-do-we-frame-the-problem-in-the-language-of-karimi2019non" title="Permalink to this headline">#</a></h2>
<p>To prove the convergence of our <a class="reference internal" href="../2020-12-01-biased-stochastic-approximation/"><span class="doc std std-doc">biased</span></a> stochastic approximation, we cannot apply the standard unbiased stochastic approximation theory of Robbins and Monro. We can however apply the work of <span id="id3">[<a class="reference internal" href="../2021-06-05-spiking-neural-networks/#id50" title="Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, 1944–1974. PMLR, 2019.">KMMW19</a>]</span> which gives some guarantees for biased stochastic approximation involving Markov updates. In this section, we will now derive sufficient conditions for the <span class="xref myst">convergence</span> of our biased stochastic approximation.</p>
<p>We <span class="xref myst">recall</span> some key assumptions about our disciminative model <span class="math notranslate nohighlight">\(\{Q_\lambda:\lambda\in\Lambda\},\)</span> generative model <span class="math notranslate nohighlight">\(\{P_\theta:\theta \in \Theta\}\)</span> and true distribution <span class="math notranslate nohighlight">\(Q_*.\)</span></p>
<hr class="docutils" />
<p><strong>C1 (Markov property and stationarity).</strong> Under the true distribution <span class="math notranslate nohighlight">\(Q_*,\)</span> the process <span class="math notranslate nohighlight">\(\{X_t\}\)</span> is Markov. Under each <span class="math notranslate nohighlight">\(Q_\lambda\)</span> and each <span class="math notranslate nohighlight">\(P_\theta,\)</span> the process <span class="math notranslate nohighlight">\(\{(Z_t, X_t)\}\)</span> is Markov, and <span class="math notranslate nohighlight">\(Z_t\)</span> and <span class="math notranslate nohighlight">\(X_t\)</span> are independent given their past. The true distribution <span class="math notranslate nohighlight">\(Q_*\)</span> has a stationary distribution <span class="math notranslate nohighlight">\(\bar{\pi}_*,\)</span> and each <span class="math notranslate nohighlight">\(Q_\lambda\)</span> has a stationary distribution <span class="math notranslate nohighlight">\(\bar{\pi}_\lambda.\)</span></p>
<hr class="docutils" />
<p>First, let <span class="math notranslate nohighlight">\(W_n \in \mathcal{W}\)</span> denote</p>
<div class="math notranslate nohighlight">
\[W_n = (W_{n1}, W_{n2}, W_{n3}, W_{n4}, W_{n5}) := (Z_n,X_n,Z_{n-1},X_{n-1}, \alpha_n).\]</div>
<p>Then <span class="math notranslate nohighlight">\(W_n\)</span> is a <span class="math notranslate nohighlight">\(\lambda\)</span>-controlled Markov process. Abusing notation, we write the distribution of <span class="math notranslate nohighlight">\(W_n\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; \displaystyle 
Q_\lambda(W_{n+1} \vert W_n) 
\\ &amp; \\ &amp; = \displaystyle
Q_\lambda\left((\,Z_{n+1},X_{n+1},Z_n,X_n,\alpha_{n+1})\, \vert\, (Z_n,X_n,Z_{n-1},X_{n-1},\alpha_n) \, \right)
\\ &amp; \\ &amp; = \displaystyle
Q_*(X_{n+1}\vert X_n) Q_\lambda(Z_{n+1}\vert Z_n, X_n)
\end{array} \end{split}\]</div>
<p>where we require</p>
<div class="math notranslate nohighlight">
\[\alpha_{n+1} = \alpha_n + \frac{d}{d\lambda} \log Q_{\lambda}(Z_{n+1} \vert  Z_{n},X_{n}).\]</div>
<p>Our Lyapunov function is the learning objective</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
V(\lambda,\theta) := I_{\bar{Q}_\lambda \Vert P_\theta}(Z_1, X_1 \vert Z_0, X_0).\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{Q}_\lambda\)</span> is the Markov chain with initial distribution <span class="math notranslate nohighlight">\(\bar{\pi}_\lambda\)</span> and the same transition probabilities as <span class="math notranslate nohighlight">\(Q_\lambda.\)</span></p>
<p>Note that this Lyapunov function is bounded below by zero, because it is a form of relative information. We assume some standard regularity condition on <span class="math notranslate nohighlight">\(V(\lambda,\theta).\)</span></p>
<hr class="docutils" />
<p><strong>C2 (<span class="math notranslate nohighlight">\(\ell\)</span>-smoothness).</strong> There exists <span class="math notranslate nohighlight">\(\ell &lt; \infty\)</span> such that for all <span class="math notranslate nohighlight">\(\lambda,\lambda' \in \Lambda\)</span> and <span class="math notranslate nohighlight">\(\theta, \theta' \in \Theta,\)</span></p>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\left\Vert \frac{\partial V}{\partial \theta}(\lambda,\theta) - \frac{\partial V}{\partial \theta}(\lambda,\theta') \right\Vert \leq \ell \Vert \theta - \theta' \Vert,\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\left\Vert \frac{\partial V}{\partial \lambda}(\lambda,\theta) - \frac{\partial V}{\partial \lambda}(\lambda',\theta) \right\Vert \leq \ell \Vert \lambda - \lambda' \Vert.\]</div>
<hr class="docutils" />
<p>We write the stochastic updates as</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} 
W_{n+1} &amp;\sim Q_{\lambda_{n}}(W_{n+1} \vert W_n).
\\ &amp; \\
\displaystyle \theta_{n+1} &amp; 
\displaystyle = \theta_n - \eta_{n+1} G_\theta(W_{n+1};\lambda_n,\theta_n)
\\ &amp; \\ 
\displaystyle \lambda_{n+1} &amp; 
\displaystyle = \lambda_n - \eta_{n+1} G_\lambda(W_{n+1};\lambda_n,\theta_{n})
\end{array} \end{split}\]</div>
<p>where we have</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} 
\displaystyle G_\theta(W_{n+1};\lambda_n,\theta_n) &amp;= 
\displaystyle - \left. \frac{d}{d\theta} \log P_\theta(Z_{n+1}, X_{n+1} \vert Z_{n},X_{n}) \right\vert _{\theta = \theta_n}
\\ &amp; \\
\displaystyle G_\lambda(W_{n+1};\lambda_n,\theta_{n}) &amp;= 
\displaystyle \alpha_{n+1}\left( \log \frac{Q_{\lambda_n}(Z_{n+1}\vert Z_{n},X_{n})}{P_{\theta_{n}}(Z_{n+1},X_{n+1}\vert Z_{n},X_{n})} +\xi(X_{n+1} \vert X_n) \right)
. \end{array}\end{split}\]</div>
<p>The mean fields of the updates are given by</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} 
\displaystyle g_\theta(\lambda_n, \theta_n) &amp; =
\displaystyle - \lim_{T\rightarrow \infty} \mathbb{E}_{Q_{\lambda_n}} \left[ \left.\frac{d}{d\theta} \log P_\theta(Z_{T+1}, X_{T+1} \vert Z_{T},X_{T})\right\vert _{\theta = \theta_n} \right]
\\ &amp; \\ &amp; =
\displaystyle \frac{\partial V}{\partial\theta}(\lambda_n,\theta_n) 
\\ &amp; \\
\displaystyle g_\lambda(\lambda_n,\theta_{n}) &amp; =
\displaystyle
\lim_{T\rightarrow \infty} \mathbb{E}_{Q_{\lambda_n}} \Bigg[ \left( \log \frac{Q_{\lambda_n}(Z_{T+1}\vert Z_{T},X_{T})}{P_{\theta_{n}}(Z_{T+1},X_{T+1}\vert Z_{T},X_{T})} +\xi(X_{T+1} \vert X_T)\right) 
\\ &amp; \\ &amp; \quad\quad \displaystyle \times \sum_{t=0}^{T} \left. \frac{d}{d\lambda} \log Q_\lambda(Z_{t+1} \vert  Z_{t},X_{t}) \right\vert_{\lambda=\lambda_n} \Bigg]
\\ &amp; \\ &amp; =
\displaystyle
\lim_{T\rightarrow \infty} \mathbb{E}_{Q_{\lambda_n}} \Bigg[ \left( \log \frac{Q_{\lambda_n}(Z_{T+1},X_{T+1}\vert Z_{T},X_{T})}{P_{\theta_{n}}(Z_{T+1},X_{T+1}\vert Z_{T},X_{T})} -\log Q_*(X_{T+1}\vert X_T) + \xi(X_{T+1} \vert X_T)\right)  
\\ &amp; \\ &amp; \quad\quad 
\displaystyle \times \sum_{t=0}^{T} \left. \frac{d}{d\lambda} \log Q_\lambda(Z_{t+1} \vert  Z_{t},X_{t}) \right\vert_{\lambda=\lambda_n} \Bigg]
\\ &amp; \\ &amp; =
\displaystyle \frac{\partial V}{\partial\lambda}(\lambda_n,\theta_{n}) 
+ \lim_{T\rightarrow \infty} \mathbb{E}_{Q_{\lambda_n}} \Bigg[ \left( -\log Q_*(X_{T+1}\vert X_T) + \xi(X_{T+1} \vert X_T) \right) 
\\ &amp; \\ &amp; \quad \quad
\displaystyle \times \sum_{t=0}^{T} \left. \frac{d}{d\lambda} \log Q_\lambda(Z_{t+1} \vert  Z_{t},X_{t}) \right\vert_{\lambda=\lambda_n} \Bigg]
\\ &amp; \\ &amp; =
\displaystyle \frac{\partial V}{\partial\lambda}(\lambda_n,\theta_{n+1}) 
. \end{array}\end{split}\]</div>
<p>where the last equality follows from a <span class="xref myst">formula</span> for integrals over the derivative of a stationary distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp;
\displaystyle \lim_{T\rightarrow \infty} \mathbb{E}_{Q_{\lambda_n}} \Bigg[ \left( -\log Q_*(X_{T+1}\vert X_T) + \xi(X_{T+1} \vert X_T) \right) \sum_{t=0}^{T} \left. \frac{d}{d\lambda} \log Q_\lambda(Z_{t+1} \vert  Z_{t},X_{t}) \right\vert_{\lambda=\lambda_n} \Bigg]
\\ &amp; \\ &amp; =
\displaystyle \frac{\partial }{\partial\lambda} \int \bar{\pi}_\lambda(dZ_0,dX_0) Q_\lambda(dZ_1,dX_1\vert Z_0,X_0) \left( -\log Q_*(X_{1}\vert X_0) + \xi(X_1 \vert X_0) \right)
\\ &amp; \\ &amp; =
\displaystyle \frac{\partial }{\partial\lambda} \int \bar{\pi}_*(dX_0) \bar{\pi}_\lambda(dZ_0\vert X_0) Q_*(dX_1\vert X_0)Q_\lambda(dZ_1\vert Z_0,X_0) \left( -\log Q_*(X_{1}\vert X_0) + \xi(X_1 \vert X_0) \right)
\\ &amp; \\ &amp; =
\displaystyle \frac{\partial }{\partial\lambda} \int \bar{\pi}_*(dX_0) Q_*(dX_1\vert X_0)\left( -\log Q_*(X_{1}\vert X_0) + \xi(X_1 \vert X_0) \right)
\\ &amp; \\ &amp; =
0 . \end{array}\end{split}\]</div>
<p>It should not be surprising that terms depending only on the <span class="math notranslate nohighlight">\(X_t\)</span> drop out of the mean field, because the mean field involves an integral over the <em>derivative</em> of a stationary distribution with respect to <span class="math notranslate nohighlight">\(\lambda\)</span> but the distribution of the <span class="math notranslate nohighlight">\(X_t\)</span> is independent of <span class="math notranslate nohighlight">\(\lambda.\)</span> The situation is similar to that of the following simplified example.</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp;
\displaystyle \int r(X) \,\frac{d }{d\lambda} \bar{\pi}_\lambda(dZ\vert X)\bar{\pi}(dX)  
\\ &amp; \\ &amp; =
\displaystyle \int \bar{\pi}(dX) r(X) \, \frac{d}{d\lambda} \int  \bar{\pi}_\lambda (dZ\vert X) 
\\ &amp; \\ &amp; =
\displaystyle \int \bar{\pi}(dX) r(X) \,  \frac{d}{d\lambda} 1 
\\ &amp; \\ &amp; =
0 . \end{array}\end{split}\]</div>
<p>From the mean fields, we have the following correction terms.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{rl} 
\displaystyle E_\theta(W_{n+1};\lambda_n,\theta_n) &amp;= 
\displaystyle G_\theta(W_{n+1};\lambda_n,\theta_n) - g_\theta(\lambda_n, \theta_n)
\\ &amp; \\
\displaystyle E_\lambda(W_{n+1};\lambda_n,\theta_{n}) &amp;= 
\displaystyle G_\lambda(W_{n+1};\lambda_n,\theta_{n}) - g_\lambda(\lambda_n,\theta_{n})
. \end{array}\end{split}\]</div>
<p>Note that the <em>total expectation</em> (with respect to the stationary distribution <span class="math notranslate nohighlight">\(\bar{\pi}_\lambda\)</span> on <span class="math notranslate nohighlight">\(W_{n+1}\)</span>) of each correction term is zero by definition. Now, if the <em>conditional expectation</em> (given the past <span class="math notranslate nohighlight">\(W_{n}, \ldots, W_1\)</span>) is zero, then the stochastic approximation is <em>unbiased</em>. Otherwise, the stochastic approximation is <em>biased</em>, which is the case for our problem.</p>
</div>
<div class="section" id="how-do-we-prove-convergence">
<h2>How do we prove convergence?<a class="headerlink" href="#how-do-we-prove-convergence" title="Permalink to this headline">#</a></h2>
<p>First, we make some common assumptions about the step sizes and the stop time for the stochastic approximation.</p>
<hr class="docutils" />
<p><strong><a id="assumption-step-sizes-and-stop-time"></a>C3 (Step sizes and stop time).</strong> For all <span class="math notranslate nohighlight">\(k \geq 1,\)</span> the step sizes are given by</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\eta_k = \eta_1 k^{-1/2}\]</div>
<p>for some fixed <span class="math notranslate nohighlight">\(\eta_1 &gt; 0.\)</span> Given a fixed time limit <span class="math notranslate nohighlight">\(T &gt; 0,\)</span> the stochastic approximation stops randomly after <span class="math notranslate nohighlight">\(N \in \{0, \ldots, T\}\)</span> steps according to the probabilities</p>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\mathbb{P}[N=k] = \eta_{k+1} \left(\sum_{j=0}^T \eta_{j+1} \right)^{-1}.\]</div>
<hr class="docutils" />
<p>Similar to a previous <span class="xref myst">analysis</span>, our goal is to find an upper bound such that</p>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\mathbb{E}\left[\left\Vert g_\theta(\lambda_N, \theta_N) \right\Vert^2\right] + \mathbb{E}\left[\left\Vert g_\lambda(\lambda_N,\theta_{N}) \right\Vert^2\right] \leq C(T) \rightarrow 0 \]</div>
<p>as <span class="math notranslate nohighlight">\(T \rightarrow \infty,\)</span> where the expectations are taken over stochastic updates in the stochastic approximation and over the random stopping time.</p>
<p>Sometimes, it is not possible to prove convergence of the above upper bound to zero because of certain relaxations (e.g. estimating the true conditional entropy) in the stochastic approximation. Instead, we may prove that the upper bound converges to some positive constant.</p>
<p>Either way, we get some control on the mean and variance of the lengths <span class="math notranslate nohighlight">\(\left\Vert g_\theta(\lambda_N, \theta_N) \right\Vert\)</span> and <span class="math notranslate nohighlight">\(\left\Vert g_\lambda(\lambda_N,\theta_{N}) \right\Vert\)</span> of the mean fields or total expectations of the stochastic updates, despite the fact that the parameters <span class="math notranslate nohighlight">\(\theta_n\)</span> and <span class="math notranslate nohighlight">\(\lambda_n\)</span> are continually fluctuating because of the stochastic updates.</p>
<p>Note that by definition</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\mathbb{E}\left[\left\Vert g_\theta(\lambda_N, \theta_N) \right\Vert^2\right] = \frac{\sum_{n=0}^T \eta_{n+1} \mathbb{E}\left[ \left\Vert g_\theta(\lambda_n, \theta_n) \right\Vert^2 \right]}{\sum_{n=0}^T \eta_{n+1}}, \]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle
\mathbb{E}\left[\left\Vert g_\lambda(\lambda_N, \theta_{N}) \right\Vert^2\right] = \frac{\sum_{n=0}^T \eta_{n+1} \mathbb{E}\left[ \left\Vert g_\lambda(\lambda_n, \theta_{n}) \right\Vert^2 \right]}{\sum_{n=0}^T \eta_{n+1}}. \]</div>
<p>Following the previous <span class="xref myst">analysis</span>, the proof starts with the <span class="math notranslate nohighlight">\(\ell\)</span>-smoothness of <span class="math notranslate nohighlight">\(V(\lambda,\theta)\)</span> which implies</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp;
\displaystyle
V(\lambda_{n+1},\theta_{n+1}) - V(\lambda_n,\theta_n) 
\\ &amp; \\ &amp; \leq  
\displaystyle -\eta_{n+1}\left\langle \frac{\partial V}{\partial\lambda}(\lambda_n,\theta_{n}) , G_\lambda(W_{n+1}; \lambda_n,\theta_{n}) \right\rangle
\\ &amp; \\ &amp; \quad
\displaystyle -\eta_{n+1}\left\langle \frac{\partial V}{\partial\theta}(\lambda_n,\theta_n) , G_\theta(W_{n+1}; \lambda_n,\theta_{n}) \right\rangle
\\ &amp; \\ &amp; \quad
\displaystyle + \frac{\ell}{2} \eta_{n+1}^2 \left( \left\Vert G_\lambda(W_{n+1}; \lambda_n,\theta_{n}) \right\Vert^2 + \left\Vert G_\theta(W_{n+1}; \lambda_n,\theta_n) \right\Vert^2 \right)
.\end{array} \end{split}\]</div>
<p>We now substitute the mean fields and correction terms. After some rearranging and summing from <span class="math notranslate nohighlight">\(n=0\)</span> to <span class="math notranslate nohighlight">\(n=T\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp;
\displaystyle
\sum_{n=0}^T \eta_{n+1}\left\langle \frac{\partial V}{\partial\lambda}(\lambda_n,\theta_{n}) , g_\lambda(\lambda_n,\theta_{n}) \right\rangle
\\ &amp; \\ &amp; +
\displaystyle
\sum_{n=0}^T \eta_{n+1}\left\langle \frac{\partial V}{\partial\theta}(\lambda_n,\theta_n) , g_\theta(\lambda_n,\theta_{n}) \right\rangle 
\\ &amp; \\ &amp; \leq  
V(\lambda_{0},\theta_{0}) - V(\lambda_{T+1},\theta_{T+1})
\\ &amp; \\ &amp; \quad
\displaystyle -\eta_{n+1}\left\langle \frac{\partial V}{\partial\lambda}(\lambda_n,\theta_{n}) , \sum_{n=0}^T E_\lambda(W_{n+1}; \lambda_n,\theta_{n}) \right\rangle
\\ &amp; \\ &amp; \quad
\displaystyle -\eta_{n+1}\left\langle \frac{\partial V}{\partial\theta}(\lambda_n,\theta_n) , \sum_{n=0}^T E_\theta(W_{n+1}; \lambda_n,\theta_{n}) \right\rangle
\\ &amp; \\ &amp; \quad
\displaystyle + \frac{\ell}{2} \eta_{n+1}^2 \sum_{n=0}^T \left( \left\Vert g_\lambda(\lambda_n,\theta_{n}) \right\Vert^2 + \left\Vert E_\lambda(W_{n+1}; \lambda_n,\theta_{n}) \right\Vert^2 \right)
\\ &amp; \\ &amp; \quad
\displaystyle + \frac{\ell}{2} \eta_{n+1}^2 \sum_{n=0}^T \left( \left\Vert g_\theta(\lambda_n,\theta_{n}) \right\Vert^2 + \left\Vert E_\theta(W_{n+1}; \lambda_n,\theta_n) \right\Vert^2 \right)
.\end{array} \end{split}\]</div>
<p>The left-hand-side of the inequality can be bounded below by some affine function of <span class="math notranslate nohighlight">\(\left\Vert g_\lambda(\lambda_n,\theta_{n}) \right\Vert^2\)</span> and <span class="math notranslate nohighlight">\( \left\Vert g_\theta(\lambda_n,\theta_{n}) \right\Vert^2.\)</span> On the right-hand-side of the inequality, we assume that the Lyapunov difference <span class="math notranslate nohighlight">\(V(\lambda_{0},\theta_{0}) - V(\lambda_{T+1},\theta_{T+1})\)</span> and the squared corrections <span class="math notranslate nohighlight">\(\left\Vert E_\lambda(W_{n+1}; \lambda_n,\theta_{n}) \right\Vert^2\)</span> and <span class="math notranslate nohighlight">\(\left\Vert E_\theta(W_{n+1}; \lambda_n,\theta_n) \right\Vert^2\)</span> are bounded above by constants.</p>
<hr class="docutils" />
<p><strong>C4 (Correction bound).</strong> There exists <span class="math notranslate nohighlight">\(\sigma &lt; \infty\)</span> such that for all <span class="math notranslate nohighlight">\(\lambda \in \Lambda, \theta \in \Theta, w \in \mathcal{W},\)</span></p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\Vert E_\lambda(w; \lambda,\theta) \Vert \leq \sigma,\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle
\Vert E_\theta(w; \lambda,\theta) \Vert \leq \sigma.\]</div>
<hr class="docutils" />
<p>Overall, if we also have control over the terms</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\left\langle \frac{\partial V}{\partial\lambda}(\lambda_n,\theta_{n}) , \sum_{n=0}^T E_\lambda(W_{n+1}; \lambda_n,\theta_{n}) \right\rangle, \]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle
\left\langle \frac{\partial V}{\partial\theta}(\lambda_n,\theta_n) , \sum_{n=0}^T E_\theta(W_{n+1}; \lambda_n,\theta_{n}) \right\rangle, \]</div>
<p>then we can bound <span class="math notranslate nohighlight">\(\sum_{n=0}^T \eta_{n+1}\left\Vert g_\lambda(\lambda_n,\theta_{n}) \right\Vert^2\)</span> and <span class="math notranslate nohighlight">\(\sum_{n=0}^T \eta_{n+1} \left\Vert g_\theta(\lambda_n,\theta_{n}) \right\Vert^2,\)</span> which in turn gives us a bound on <span class="math notranslate nohighlight">\( \mathbb{E}[\left\Vert g_\theta(\lambda_N, \theta_N) \right\Vert^2] + \mathbb{E}[\left\Vert g_\lambda(\lambda_N,\theta_{N}) \right\Vert^2].\)</span></p>
<p>Bounding the above two terms will require solutions of the Poisson equations for <span class="math notranslate nohighlight">\(E_\lambda(W_{n+1}; \lambda_n,\theta_{n})\)</span> and <span class="math notranslate nohighlight">\(E_\theta(W_{n+1}; \lambda_n,\theta_{n}).\)</span></p>
</div>
<div class="section" id="what-are-the-corresponding-poisson-equations">
<h2>What are the corresponding Poisson equations?<a class="headerlink" href="#what-are-the-corresponding-poisson-equations" title="Permalink to this headline">#</a></h2>
<p>We study the first Poisson equation</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
L_{\lambda} \hat{E}_{\theta} (w; \lambda,\theta) = E_{\theta}(w; \lambda,\theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(w = (z_1, x_1, z_0, x_0,\alpha_1),\)</span></p>
<div class="math notranslate nohighlight">
\[ \displaystyle
L_{\lambda} \hat{E}_{\theta} (w;\lambda,\theta) = Q_{\lambda} \hat{E}_{\theta} (w;\lambda,\theta)- \hat{E}_{\theta} (w;\lambda,\theta).\]</div>
<p>A candidate solution (if it is well-defined) of the Poisson equation is</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\hat{E}_{\theta} (w;\lambda,\theta) = - \displaystyle \sum_{t=0}^\infty Q_{\lambda}^t E_{\theta}(w;\lambda,\theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(Q^t_\lambda\)</span> denotes <span class="math notranslate nohighlight">\(t\)</span> applications of the Markov kernel <span class="math notranslate nohighlight">\(Q_\lambda.\)</span></p>
<p>Now, for all <span class="math notranslate nohighlight">\(t \geq 1,\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle Q_\lambda^t G_{\theta}(w;\lambda,\theta) 
\\ &amp; \\ &amp; = 
\displaystyle \frac{\partial}{\partial\theta} \int \hat{Q}_\lambda(dZ_0,dX_0)\hat{Q}_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) \log \frac{\hat{Q}_\lambda(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})}{P_\theta(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})} 
\\ &amp; \\ &amp; = 
\displaystyle \frac{\partial}{\partial\theta}  I_{\hat{Q}_\lambda \Vert P_\theta}(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})  \end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{Q}_\lambda\)</span> is the distribution of the Markov chain that initializes <span class="math notranslate nohighlight">\((Z_0,X_0)\)</span> with the state <span class="math notranslate nohighlight">\((z_1, x_1)\)</span> and has transition probabilities <span class="math notranslate nohighlight">\(Q_\lambda.\)</span> Therefore,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle
Q_{\lambda} \hat{E}_{\theta} (w;\lambda,\theta)
\\ &amp; \\ &amp; = 
\displaystyle
-\lim_{T \rightarrow \infty} \sum_{t=1}^{T} Q_\lambda^{t} E_{\theta}(w;\lambda,\theta)  
\\ &amp; \\ &amp; = 
\displaystyle
- \lim_{T \rightarrow \infty} \sum_{t=1}^{T} Q_\lambda^{t} \left(G_{\theta}(w;\lambda,\theta)-g_{\theta}(w;\lambda,\theta)  \right)
\\ &amp; \\ &amp; = 
\displaystyle
\lim_{T \rightarrow \infty} \sum_{t=1}^{T} g_{\theta}(w;\lambda,\theta)- Q_\lambda^{t}G_{\theta}(w;\lambda,\theta) 
\\ &amp; \\ &amp; = 
\displaystyle \lim_{T \rightarrow \infty} \frac{\partial}{\partial\theta} \Big( \sum_{t=1}^{T} I_{\bar{Q}_\lambda \Vert P_\theta}(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})
 - \sum_{t=1}^{T}  I_{\hat{Q}_\lambda \Vert P_\theta}(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1}) \Big) 
 \\ &amp; \\ &amp; = 
 \displaystyle \lim_{T \rightarrow \infty} \frac{\partial}{\partial\theta} \Big( I_{\bar{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0}) -  I_{\hat{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0}) \Big) 
\end{array}\end{split}\]</div>
<p>where the last equality follows from the chain rule for conditional relative information.</p>
<p>As for the second Poisson equation</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
L_{\lambda} \hat{E}_{\lambda} (w; \lambda,\theta) = E_{\lambda}(w; \lambda,\theta)\]</div>
<p>a candidate solution is</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\hat{E}_{\lambda} (w;\lambda,\theta) = - \displaystyle \sum_{t=0}^\infty Q_{\lambda}^t E_{\lambda}(w;\lambda,\theta).\]</div>
<p>Now, as shown in the <span class="xref myst">appendix</span>, for all <span class="math notranslate nohighlight">\(t \geq 1,\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle Q_\lambda^t G_{\lambda}(w;\lambda,\theta) 
\\ &amp; \\ &amp; = 
\left( I_{\hat{Q}_\lambda\Vert P_\theta}(Z_t,X_t\vert Z_{t-1},X_{t-1}) + H_{\hat{Q}_*}(X_t\vert X_{t-1}) + \xi_{\hat{Q}_*}(X_t \vert X_{t-1}) \right) \alpha_1
\\ &amp; \\ &amp; \quad 
\displaystyle + \frac{\partial}{\partial \lambda} I_{\hat{Q}_\lambda\Vert P_\theta}(Z_t,X_t\vert Z_{t-1},X_{t-1})
. \end{array}\end{split}\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle
Q_{\lambda} \hat{E}_{\lambda} (w;\lambda,\theta)
\\ &amp; \\ &amp; = 
\displaystyle
\lim_{T \rightarrow \infty} \sum_{t=1}^{T} g_{\lambda}(w;\lambda,\theta)- Q_\lambda^{t}G_{\lambda}(w;\lambda,\theta) 
\\ &amp; \\ &amp; = 
\displaystyle \lim_{T \rightarrow \infty} \frac{\partial}{\partial\lambda} \Big( \sum_{t=1}^{T} I_{\bar{Q}_\lambda \Vert P_\theta}(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})
 - \sum_{t=1}^{T}  I_{\hat{Q}_\lambda \Vert P_\theta}(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1}) \Big) 
 \\ &amp; \\ &amp; \quad \quad
 \displaystyle - \left( \sum_{t=1}^{T}  I_{\hat{Q}_\lambda\Vert P_\theta}(Z_t,X_t\vert Z_{t-1},X_{t-1}) + \sum_{t=1}^{T} H_{\hat{Q}_*}(X_t\vert X_{t-1}) + \sum_{t=1}^T \xi_{\hat{Q}_*}(X_t \vert X_{t-1}) \right) \alpha_1
 \\ &amp; \\ &amp; = 
 \displaystyle \lim_{T \rightarrow \infty} \frac{\partial}{\partial\lambda} \Big( I_{\bar{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0}) -  I_{\hat{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0}) \Big) 
 \\ &amp; \\ &amp; \quad \quad
 \displaystyle - \left( I_{\hat{Q}_\lambda\Vert P_\theta}(Z_{[1..T]},X_{[1..T]}\vert Z_0,X_0) +  H_{\hat{Q}_*}(X_{[1..T]}\vert X_0) + \xi_{\hat{Q}_*}(X_{[1..T]} \vert X_{0})\right) \alpha_1
\end{array}\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\xi_{\hat{Q}_*}(X_{[1..T]} \vert X_{0}) = \sum_{t=1}^T \xi_{\hat{Q}_*}(X_t \vert X_{t-1}).\]</div>
<p>Bringing them all together,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl}
V(\lambda,\theta) &amp;= I_{\bar{Q}_\lambda \Vert P_\theta}(Z_1, X_1 \vert Z_0, X_0) 
\\ &amp; \\ 
g_\theta(\lambda, \theta) &amp; = 
\displaystyle \frac{\partial V}{\partial \theta}(\lambda, \theta) 
\\ &amp; \\ 
g_\lambda(\lambda, \theta) &amp; = 
\displaystyle \frac{\partial V}{\partial \lambda}(\lambda, \theta) 
\\ &amp; \\ 
E_{\theta} (w;\lambda,\theta) &amp; = 
\displaystyle - \frac{\partial}{\partial\theta} \Big( V(\lambda,\theta)+ \log P_\theta(z_1, x_1 \vert z_0, x_0) \Big) 
\\ &amp; \\ 
E_\lambda(w;\lambda,\theta) &amp;= 
\displaystyle \alpha_1 \left( \log \frac{Q_{\lambda}(z_1\vert z_0,x_0)}{P_{\theta}(z_1,x_1\vert z_0,x_0)} -\xi(x_1 \vert x_0) \right) - \frac{\partial V}{\partial \lambda}(\lambda, \theta) 
\\ &amp; \\
Q_{\lambda} \hat{E}_{\theta} (w;\lambda,\theta) &amp; =
\displaystyle \lim_{T \rightarrow \infty} \frac{\partial}{\partial\theta} \Big( I_{\bar{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0})  
-  I_{\hat{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0}) \Big)
\\ &amp; \\ 
Q_{\lambda} \hat{E}_{\lambda} (w;\lambda,\theta) &amp; = 
 \displaystyle \lim_{T \rightarrow \infty} \frac{\partial}{\partial\lambda} \Big( I_{\bar{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0}) -  I_{\hat{Q}_\lambda \Vert P_\theta}(Z_{[1..T]}, X_{[1..T]} \vert Z_{0}, X_{0}) \Big) 
 \\ &amp; \\ &amp; \quad \quad
 \displaystyle - \left( I_{\hat{Q}_\lambda\Vert P_\theta}(Z_{[1..T]},X_{[1..T]}\vert Z_0,X_0) +  H_{\hat{Q}_*}(X_{[1..T]}\vert X_0) + \xi_{\hat{Q}_*}(X_{[1..T]} \vert X_{0}) \right) \alpha_1
\\ &amp; \\ 
\hat{E}_{\theta} (w;\lambda,\theta) &amp; = 
Q_{\lambda} \hat{E}_{\theta} (w;\lambda,\theta) - E_{\theta} (w;\lambda,\theta)
\\ &amp; \\ 
\hat{E}_{\lambda} (w;\lambda,\theta) &amp; = 
Q_{\lambda} \hat{E}_{\lambda} (w;\lambda,\theta) - E_{\lambda} (w;\lambda,\theta) 
\end{array}\end{split}\]</div>
<p>We impose the following regularity conditions.</p>
<hr class="docutils" />
<p><strong>C5 (Solution of Poisson equation).</strong> The limits</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
Q_\lambda\hat{E}_\lambda : \mathcal{W} \times \Lambda \times \Theta \rightarrow \Lambda \]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle
Q_\lambda\hat{E}_\theta : \mathcal{W} \times \Lambda \times \Theta \rightarrow \Theta\]</div>
<p>are well-defined and measurable.</p>
<p><strong>C6 (Regularity of solution).</strong> There exists <span class="math notranslate nohighlight">\(\ell_0, \ell_1 &lt; \infty\)</span> such that for all <span class="math notranslate nohighlight">\(\lambda,\lambda' \in \Lambda\)</span> and <span class="math notranslate nohighlight">\(\theta, \theta' \in \Theta\)</span> and <span class="math notranslate nohighlight">\(w \in \mathcal{W},\)</span></p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\Vert \hat{E}_{\lambda} (w;\lambda,\theta) \Vert \leq \ell_0, \quad \Vert Q_\lambda \hat{E}_{\lambda} (w;\lambda,\theta)  \Vert \leq \ell_0,\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle
\Vert \hat{E}_{\theta} (w;\lambda,\theta) \Vert \leq \ell_0, \quad \Vert Q_\lambda \hat{E}_{\theta} (w;\lambda,\theta)  \Vert \leq \ell_0,\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle
\Vert Q_\lambda \hat{E}_{\lambda} (w;\lambda,\theta) -Q_{\lambda'} \hat{E}_{\lambda} (w;\lambda',\theta)  \Vert \leq \ell_1 \Vert \lambda - \lambda' \Vert,\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle
\Vert Q_\lambda \hat{E}_{\lambda} (w;\lambda,\theta) -Q_{\lambda} \hat{E}_{\lambda} (w;\lambda,\theta')  \Vert \leq \ell_1 \Vert \theta - \theta' \Vert.\]</div>
<hr class="docutils" />
<p><strong><a id="theorem-convergence-of-online-learning"></a>Theorem (Convergence of online learning).</strong> Suppose that we have stochastic approximation</p>
<div class="math notranslate nohighlight">
\[ \displaystyle 
X_{n+1} \sim Q_*(X_{n+1} \vert X_{n})\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
Z_{n+1} \sim Q_{\lambda_{n}}(Z_{n+1} \vert Z_{n}, X_{n})\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\theta_{n+1} = \theta_{n} + \eta_{n+1} \left.\frac{d}{d\theta} \log P_\theta(Z_{n+1}, X_{n+1} \vert Z_{n},X_{n}) \right\vert _{\theta = \theta_{n}}\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\alpha_{n+1} = \alpha_{n} + \left.\frac{d}{d\lambda} \log Q_{\lambda}(Z_{n+1} \vert  Z_{n},X_{n})\right\vert _{\lambda=\lambda_{n}}\]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\gamma_{n+1} = \xi(X_{n+1} \vert X_n) + \log \frac{Q_{\lambda_{n}}(Z_{n+1}\vert Z_{n},X_{n})}{P_{\theta_{n}}(Z_{n+1},X_{n+1}\vert Z_{n},X_{n})} \]</div>
<div class="math notranslate nohighlight">
\[ \displaystyle 
\lambda_{n+1} = \lambda_{n} - \eta_{n+1} \alpha_{n+1} \gamma_{n+1}\]</div>
<p>Then assuming C1-C6 and sufficiently small initial step size <span class="math notranslate nohighlight">\(\eta_1,\)</span>
we have</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\mathbb{E}\left[\left\Vert g_\theta(\lambda_N, \theta_N) \right\Vert^2\right] + \mathbb{E}\left[\left\Vert g_\lambda(\lambda_N,\theta_{N}) \right\Vert^2\right] = O(\log T / \sqrt{T} )\]</div>
<p>where the maximum time <span class="math notranslate nohighlight">\(T\)</span> was defined in <span class="xref myst">C3</span>.</p>
</div>
<hr class="docutils" />
<div class="section" id="appendix-poisson-equation-for-discriminative-model-update">
<h2>Appendix : Poisson equation for discriminative model update<a class="headerlink" href="#appendix-poisson-equation-for-discriminative-model-update" title="Permalink to this headline">#</a></h2>
<p>In this section, we derive the terms <span class="math notranslate nohighlight">\(Q_\lambda^t G_{\lambda}(w;\lambda,\theta)\)</span> appearing in our candidate solution to the Poisson equation for the discriminative model update.</p>
<p>Given <span class="math notranslate nohighlight">\(w = (z_1, x_1, z_0, x_0,\alpha_1),\)</span> for all <span class="math notranslate nohighlight">\(t \geq 1\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle Q_\lambda^t G_{\lambda}(w;\lambda,\theta) 
\\ &amp; \\ &amp; = 
\displaystyle \int \hat{Q}_\lambda(dZ_0,dX_0)Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) 
\\ &amp; \\ &amp; \quad \quad 
\displaystyle \times \left(\log \frac{Q_\lambda(Z_{t}, X_t \vert Z_{t-1}, X_{t-1})}{P_\theta(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})} - \log Q_*(X_t\vert X_{t-1}) +\xi(X_{t} \vert X_{t-1})\right)
\\ &amp; \\ &amp; \quad \quad 
\displaystyle \times \left( \alpha_1 + \sum_{s=1}^t \frac{d}{d\lambda} \log Q_\lambda(Z_{s}\vert Z_{s-1}, X_{s-1} ) \right)
. \end{array}\end{split}\]</div>
<p>We observe that</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle \int \hat{Q}_\lambda(dZ_0,dX_0) Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) \left(\log \frac{Q_\lambda(Z_{t}, X_t \vert Z_{t-1}, X_{t-1})}{P_\theta(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})} \right) 
\\ &amp; \\ &amp; = 
\displaystyle I_{\hat{Q}_\lambda\Vert P_\theta}(Z_t,X_t\vert Z_{t-1},X_{t-1})
\end{array}\end{split}\]</div>
<p>and that</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle \int \hat{Q}_\lambda(dZ_0,dX_0) Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) \left(- \log Q_*(X_t\vert X_{t-1}) +\xi(X_{t} \vert X_{t-1}) \right) 
\\ &amp; \\ &amp; = 
\displaystyle
\int \hat{Q}_*(dX_0) Q_*^t(dX_t\vert X_0) \left(- \log Q_*(X_t\vert X_{t-1}) +\xi(X_{t} \vert X_{t-1}) \right) 
\\ &amp; \\ &amp; =
H_{\hat{Q}_*}(X_t\vert X_{t-1}) + \xi_{\hat{Q}_*}(X_{t} \vert X_{t-1})
. \end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{Q}_*\)</span> is the distribution of the Markov chain that initializes <span class="math notranslate nohighlight">\(X_0\)</span> with the state <span class="math notranslate nohighlight">\(x_1\)</span> and has transition probabilities <span class="math notranslate nohighlight">\(Q_*,\)</span> <span class="math notranslate nohighlight">\(H_{\hat{Q}_*}\)</span> is the <span class="xref myst">conditional entropy</span>, and</p>
<div class="math notranslate nohighlight">
\[ \displaystyle
\xi_{\hat{Q}_*}(X_{t} \vert X_{t-1}) = \mathbb{E}_{\hat{Q}_*}\left[\xi(X_{t} \vert X_{t-1})\right]. \]</div>
<p>We also note that</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp;
\displaystyle
\frac{\partial}{\partial \lambda} I_{\hat{Q}_\lambda\Vert P_\theta}(Z_t,X_t\vert Z_{t-1},X_{t-1})
\\ &amp; \\ &amp; =
\displaystyle \int \hat{Q}_\lambda(dZ_0,dX_0) \left(\frac{\partial}{\partial \lambda}Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) \right) \log \frac{Q_\lambda(Z_{t}, X_t \vert Z_{t-1}, X_{t-1})}{P_\theta(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})} 
\\ &amp; \\ &amp; \quad
\displaystyle + \int \hat{Q}_\lambda(dZ_0,dX_0) Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) \left(\frac{\partial}{\partial \lambda} \log \frac{Q_\lambda(Z_{t}, X_t \vert Z_{t-1}, X_{t-1})}{P_\theta(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})} \right)
\\ &amp; \\ &amp; =
\displaystyle \int \hat{Q}_\lambda(dZ_0,dX_0)  \left(Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0)\sum_{s=1}^t \frac{d}{d\lambda} \log Q_\lambda(Z_{s},X_s\vert Z_{s-1}, X_{s-1} ) \right)
\\ &amp; \\ &amp; \quad \quad 
\displaystyle \times 
\left(\log \frac{Q_\lambda(Z_{t}, X_t \vert Z_{t-1}, X_{t-1})}{P_\theta(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})} \right)
\\ &amp; \\ &amp; \quad
\displaystyle + \int \hat{Q}_\lambda(dZ_0,dX_0) Q_\lambda^{t-1}(dZ_{t-1}, dX_{t-1} \vert Z_0, X_0) \left(\frac{d}{d \lambda} \int Q_\lambda(dZ_{t}, dX_t \vert Z_{t-1}, X_{t-1}) \right)
\\ &amp; \\ &amp; =
\displaystyle \int \hat{Q}_\lambda(dZ_0,dX_0)Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) \left(\log \frac{Q_\lambda(Z_{t}, X_t \vert Z_{t-1}, X_{t-1})}{P_\theta(Z_{t}, X_{t} \vert Z_{t-1}, X_{t-1})} \right)
\\ &amp; \\ &amp; \quad \quad 
\displaystyle \times \left(\sum_{s=1}^t \frac{d}{d\lambda} \log Q_\lambda(Z_{s}\vert Z_{s-1}, X_{s-1} ) \right)
. \end{array}\end{split}\]</div>
<p>Lastly,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp;
\displaystyle \int \hat{Q}_\lambda(dZ_0,dX_0) \left( Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0) \sum_{s=1}^t \frac{d}{d\lambda} \log Q_\lambda(Z_{s}\vert Z_{s-1}, X_{s-1} ) \right)
\\ &amp; \\ &amp; \quad \quad 
\displaystyle \times \left( - \log Q_*(X_t\vert X_{t-1}) +\xi(X_t \vert X_{t-1})\right)
\\ &amp; \\ &amp; =
\displaystyle 
\frac{d}{d\lambda} \int
\hat{Q}_\lambda(dZ_0,dX_0)  Q_\lambda^t(dZ_{t}, dX_{t} \vert Z_0, X_0)  \left( - \log Q_*(X_t\vert X_{t-1})+\xi(X_t \vert X_{t-1})\right)
\\ &amp; \\ &amp; =
\displaystyle 
\frac{d}{d\lambda} \int
\hat{Q}_*(dX_0)  Q_*^t( dX_{t} \vert X_0)  \left( - \log Q_*(X_t\vert X_{t-1}) +\xi(X_t \vert X_{t-1})\right)
\\ &amp; \\ &amp; =
0 . \end{array}\end{split}\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{array}{rl} &amp; 
\displaystyle Q_\lambda^t G_{\lambda}(w;\lambda,\theta) 
\\ &amp; \\ &amp; = 
\left( I_{\hat{Q}_\lambda\Vert P_\theta}(Z_t,X_t\vert Z_{t-1},X_{t-1}) + H_{\hat{Q}_*}(X_t\vert X_{t-1}) + \xi_{\hat{Q}_*}(X_t \vert X_{t-1}) \right) \alpha_1
\\ &amp; \\ &amp; \quad 
\displaystyle + \frac{\partial}{\partial \lambda} I_{\hat{Q}_\lambda\Vert P_\theta}(Z_t,X_t\vert Z_{t-1},X_{t-1})
. \end{array}\end{split}\]</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id4">
<dl class="citation">
<dt class="label" id="id38"><span class="brackets">KMMW19</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>)</span></dt>
<dd><p>Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In <em>Conference on Learning Theory</em>, 1944–1974. PMLR, 2019.</p>
</dd>
</dl>
</div>
</div>
</div>

<div class="section">
    

<div class="section">
  <span style="float: left">
     
    <a href="../2021-05-10-path-integrals-and-the-dyson-formula/">
      <i class="fa fa-arrow-circle-left"></i> Path integrals and the Dyson formula
    </a>
    
  </span>
  <span>&nbsp;</span>
  <span style="float: right">
     
    <a href="../2021-06-05-spiking-neural-networks/">
      Spiking neural networks <i
        class="fa fa-arrow-circle-right"
      ></i
      >
    </a>
    
  </span>
</div>
  
</div>

                </article>
              
<p style="margin-bottom:5em;"></p>
<!-- Add a comment box underneath the page's content -->
<script src="https://giscus.app/client.js"
        data-repo="shaoweilin/shaoweilin.github.io"
        data-repo-id="R_kgDOHM5tAA"
        data-category="Blog comments"
        data-category-id="DIC_kwDOHM5tAM4COqgI"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-frame-the-problem-in-the-language-of-karimi2019non">How do we frame the problem in the language of <span class="xref cite">karimi2019non</span>?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-prove-convergence">How do we prove convergence?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-corresponding-poisson-equations">What are the corresponding Poisson equations?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-poisson-equation-for-discriminative-model-update">Appendix : Poisson equation for discriminative model update</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/posts/2021-06-01-convergence-of-biased-stochastic-approximation.md.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2022, Shaowei Lin.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>