---
date: 2023-06-26
excerpts: 2
---

# All you need is relative information

## Abstract

SLT has taught me that relative information 
(or Kullback-Leibler divergence) is all you need. 
For instance, the level sets of relative information 
give us state density functions, whose Fourier, 
Laplace and Mellin transforms reveal different 
facets of learning algorithms and their ability 
to generalize. In this talk, I will outline two 
ongoing projects involving relative information. 
The first project explores the information theory 
of time series, for the purpose of understanding 
language models and reinforcement learning. Using 
relative information rate, we derive stochastic 
learning algorithms for spiking neural networks 
with memory. The second project explores cohomological 
generalizations of relative information, building on 
recent work by Pierre Baudot and Daniel Bennequin 
and by Tai-Danae Bradley. The hope is to uncover 
new tools for studying non-statistical spaces, 
by analyzing the level sets of generalized 
relative information and their transforms.

## Details
[Singular Learning Theory and Alignment Summit](https://singularlearningtheory.com/2023)

[YouTube Channel for SLTA Summit](https://www.youtube.com/@SLTSummit)

[Slides](https://w3id.org/people/shaoweilin/public/20230626-slt.pdf)

[YouTube](https://www.youtube.com/watch?v=Eg6RgDe3JQU)
