<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://shaoweilin.github.io</id>
  <title>Types from Spikes</title>
  <updated>2025-12-18T22:06:28.563821+00:00</updated>
  <link href="https://shaoweilin.github.io"/>
  <link href="https://shaoweilin.github.io/blog/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.io/" version="0.11.12">ABlog</generator>
  <entry>
    <id>https://shaoweilin.github.io/posts/2025-10-30-action-driven-processes-for-continuous-time-control/</id>
    <title>Action-driven processes for continuous-time control</title>
    <updated>2025-10-30T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;At the heart of reinforcement learning are actions – decisions made in response to observations of the environment. Actions are equally fundamental in the modeling of stochastic processes, as they trigger discontinuous state transitions and enable the flow of information through large, complex systems. In this paper, we unify the perspectives of stochastic processes and reinforcement learning through action-driven processes, and illustrate their application to spiking neural networks. Leveraging ideas from control-as-inference, we show that minimizing the Kullback-Leibler divergence between a policy-driven true distribution and a reward-driven model distribution for a suitably defined action-driven process is equivalent to maximum entropy reinforcement learning.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2025-10-30-action-driven-processes-for-continuous-time-control/"/>
    <summary>At the heart of reinforcement learning are actions – decisions made in response to observations of the environment. Actions are equally fundamental in the modeling of stochastic processes, as they trigger discontinuous state transitions and enable the flow of information through large, complex systems. In this paper, we unify the perspectives of stochastic processes and reinforcement learning through action-driven processes, and illustrate their application to spiking neural networks. Leveraging ideas from control-as-inference, we show that minimizing the Kullback-Leibler divergence between a policy-driven true distribution and a reward-driven model distribution for a suitably defined action-driven process is equivalent to maximum entropy reinforcement learning.</summary>
    <published>2025-10-30T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2025-09-26-a-benchmark-for-vericoding-formally-verified-program-synthesis/</id>
    <title>A benchmark for vericoding: formally verified program synthesis</title>
    <updated>2025-09-26T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;We present and test the largest benchmark for vericoding, LLM-generation of formally verified code from formal specifications - in contrast to vibe coding, which generates potentially buggy code from a natural language description. Our benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny using off-the-shelf LLMs. Adding natural-language descriptions does not significantly improve performance. We also find that LLM progress has improved progress on pure Dafny verification from 68% to 96% over the past year. The benchmark and vericoding results are shared at &lt;a class="reference external" href="https://github.com/Beneficial-AI-Foundation/vericoding-benchmark"&gt;this https URL&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2025-09-26-a-benchmark-for-vericoding-formally-verified-program-synthesis/"/>
    <summary>We present and test the largest benchmark for vericoding, LLM-generation of formally verified code from formal specifications - in contrast to vibe coding, which generates potentially buggy code from a natural language description. Our benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny using off-the-shelf LLMs. Adding natural-language descriptions does not significantly improve performance. We also find that LLM progress has improved progress on pure Dafny verification from 68% to 96% over the past year. The benchmark and vericoding results are shared at this https URL.</summary>
    <published>2025-09-26T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2025-05-30-refine-yourself-a-code-for-great-good/</id>
    <title>Refine yourself a code for great good!</title>
    <updated>2025-05-30T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;How do you code? Andrej Karpathy who coined the term “vibe coding,” says that for professional coding tasks, he picks single concrete incremental changes and, with AI assistance, plans them, executes them, and evaluates them. Refinement is the process of making incremental changes. It is not just the primary way we design and construct code, but also how we repair them and upgrade them, often collaboratively with other coders, designers and users.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2025-05-30-refine-yourself-a-code-for-great-good/"/>
    <summary>How do you code? Andrej Karpathy who coined the term “vibe coding,” says that for professional coding tasks, he picks single concrete incremental changes and, with AI assistance, plans them, executes them, and evaluates them. Refinement is the process of making incremental changes. It is not just the primary way we design and construct code, but also how we repair them and upgrade them, often collaboratively with other coders, designers and users.</summary>
    <published>2025-05-30T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2025-01-10-relative-information-and-the-dual-numbers/</id>
    <title>Relative information and the dual numbers</title>
    <updated>2025-01-10T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Relative information (Kullback-Leibler divergence) is a key concept in statistics, machine learning and information theory. In fact, singular learning theory tells us that the generalization error of a learning algorithm depends on the structure of algebraic geometric singularities of relative information. This leads us to wonder, what properties of relative information contribute to its ubiquity? In this talk, we will study fundamental characteristics of conditional relative information I_{q||p}(Y|X) where q, p are joint distributions on random variables X, Y. We define the rig category C of random variables and their conditional maps, as well as the rig category R(e) of dual numbers. Relative information can then be constructed, up to a scalar multiple, via rig monoidal functors from C to R(e). Closely related to this construction is the information cohomology of Baudot, Bennequin and Vigneaux.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2025-01-10-relative-information-and-the-dual-numbers/"/>
    <summary>Relative information (Kullback-Leibler divergence) is a key concept in statistics, machine learning and information theory. In fact, singular learning theory tells us that the generalization error of a learning algorithm depends on the structure of algebraic geometric singularities of relative information. This leads us to wonder, what properties of relative information contribute to its ubiquity? In this talk, we will study fundamental characteristics of conditional relative information I_{q||p}(Y|X) where q, p are joint distributions on random variables X, Y. We define the rig category C of random variables and their conditional maps, as well as the rig category R(e) of dual numbers. Relative information can then be constructed, up to a scalar multiple, via rig monoidal functors from C to R(e). Closely related to this construction is the information cohomology of Baudot, Bennequin and Vigneaux.</summary>
    <published>2025-01-10T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2024-10-16-singular-learning-relative-information-and-the-dual-numbers/</id>
    <title>Singular learning, relative information and the dual numbers</title>
    <updated>2024-10-16T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Relative information (Kullback-Leibler divergence) is a fundamental concept in statistics, machine learning and information theory. In the first half of the talk, I will define conditional relative information, list its axiomatic properties, and describe how it is used in machine learning. For example, according to Sumio Watanabe’s Singular Learning Theory, the generalization error of a learning algorithm depends on the structure of algebraic geometric singularities of relative information. In the second half of the talk, I will define the rig category Info of random variables and their conditional maps, as well as the rig category R(e) of dual numbers. Relative information can then be constructed, up to a scalar multiple, via rig monoidal functors from Info to R(e). If time permits, I may discuss how this construction relates to the information cohomology of Baudot, Bennequin and Vigneaux, and to the operad derivations of Bradley.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2024-10-16-singular-learning-relative-information-and-the-dual-numbers/"/>
    <summary>Relative information (Kullback-Leibler divergence) is a fundamental concept in statistics, machine learning and information theory. In the first half of the talk, I will define conditional relative information, list its axiomatic properties, and describe how it is used in machine learning. For example, according to Sumio Watanabe’s Singular Learning Theory, the generalization error of a learning algorithm depends on the structure of algebraic geometric singularities of relative information. In the second half of the talk, I will define the rig category Info of random variables and their conditional maps, as well as the rig category R(e) of dual numbers. Relative information can then be constructed, up to a scalar multiple, via rig monoidal functors from Info to R(e). If time permits, I may discuss how this construction relates to the information cohomology of Baudot, Bennequin and Vigneaux, and to the operad derivations of Bradley.</summary>
    <published>2024-10-16T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2024-10-01-program-synthesis/</id>
    <title>Program Synthesis</title>
    <updated>2024-10-01T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;A list of prior work I did with my collaborators and Ph.D. students on dependent type theory and program synthesis: &lt;a class="reference internal" href="../synthesis/"&gt;&lt;span class="doc std std-doc"&gt;Program Synthesis&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2024-10-01-program-synthesis/"/>
    <summary>A list of prior work I did with my collaborators and Ph.D. students on dependent type theory and program synthesis: Program Synthesis.</summary>
    <published>2024-10-01T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2024-09-24-safety-by-shared-synthesis/</id>
    <title>Safety by shared synthesis</title>
    <updated>2024-09-24T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Today, critical infrastructure is vulnerable to both malicious attacks and unintended failures, and these risks are expected to grow in the foreseeable future. Deploying formal verification (FV) across critical cyber physical systems would dramatically improve safety and security, but has historically been too costly to use outside the simplest or most critical subsystems. AI could allow widespread use of FV in years not decades, shifting cyber risks strongly in favor of defense. In this talk, I will outline our report with Atlas Computing on AI-enabled tools for scaling formal verification (https://atlascomputing.org/ai-assisted-fv-toolchain.pdf). I will also discuss some lessons that I learnt along the way, especially about shared synthesis - the collaborative construction of formal specifications, implementations and proofs.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2024-09-24-safety-by-shared-synthesis/"/>
    <summary>Today, critical infrastructure is vulnerable to both malicious attacks and unintended failures, and these risks are expected to grow in the foreseeable future. Deploying formal verification (FV) across critical cyber physical systems would dramatically improve safety and security, but has historically been too costly to use outside the simplest or most critical subsystems. AI could allow widespread use of FV in years not decades, shifting cyber risks strongly in favor of defense. In this talk, I will outline our report with Atlas Computing on AI-enabled tools for scaling formal verification (https://atlascomputing.org/ai-assisted-fv-toolchain.pdf). I will also discuss some lessons that I learnt along the way, especially about shared synthesis - the collaborative construction of formal specifications, implementations and proofs.</summary>
    <published>2024-09-24T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2024-05-22-formal-ai-assisted-code-specification-and-synthesis-concrete-steps-towards-safe-sociotechnical-systems/</id>
    <title>Formal AI-assisted code specification and synthesis: concrete steps towards safe sociotechnical systems</title>
    <updated>2024-05-22T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Atlas and Topos has been working on a roadmap for AI-assisted code specification and synthesis. The thesis is that formal verification (FV) is our best bet for protecting our sociotechnical systems, especially from human-led and/or AI-enabled attacks. Formal AI-assisted coding could make FV widespread and turn the tide in favor of defense. The roadmap describes concrete projects towards that goal. In this talk, I will discuss various components of the roadmap and show simple demos of what language models can do out of the box. I will also dive into correct-by-construction code synthesis, and how that it preferable to token-by-token code generation.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2024-05-22-formal-ai-assisted-code-specification-and-synthesis-concrete-steps-towards-safe-sociotechnical-systems/"/>
    <summary>Atlas and Topos has been working on a roadmap for AI-assisted code specification and synthesis. The thesis is that formal verification (FV) is our best bet for protecting our sociotechnical systems, especially from human-led and/or AI-enabled attacks. Formal AI-assisted coding could make FV widespread and turn the tide in favor of defense. The roadmap describes concrete projects towards that goal. In this talk, I will discuss various components of the roadmap and show simple demos of what language models can do out of the box. I will also dive into correct-by-construction code synthesis, and how that it preferable to token-by-token code generation.</summary>
    <published>2024-05-22T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2024-05-08-ai-assisted-coding-correct-by-construction-not-by-generation/</id>
    <title>AI-assisted coding: correct by construction, not by generation</title>
    <updated>2024-05-08T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Atlas and Topos has been working on a roadmap for AI-assisted code specification and synthesis. The thesis is that formal verification (FV) is our best bet for protecting our sociotechnical systems, especially from human-led and/or AI-enabled attacks. Formal AI-assisted coding could make FV widespread and turn the tide in favor of defense. The roadmap describes concrete projects towards that goal. In this talk, I will discuss various components of the roadmap and show simple demos of what language models can do out of the box. I will also dive into correct-by-construction code synthesis, and how that it preferable to token-by-token code generation.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2024-05-08-ai-assisted-coding-correct-by-construction-not-by-generation/"/>
    <summary>Atlas and Topos has been working on a roadmap for AI-assisted code specification and synthesis. The thesis is that formal verification (FV) is our best bet for protecting our sociotechnical systems, especially from human-led and/or AI-enabled attacks. Formal AI-assisted coding could make FV widespread and turn the tide in favor of defense. The roadmap describes concrete projects towards that goal. In this talk, I will discuss various components of the roadmap and show simple demos of what language models can do out of the box. I will also dive into correct-by-construction code synthesis, and how that it preferable to token-by-token code generation.</summary>
    <published>2024-05-08T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2023-12-11-online-learning-for-spiking-neural-networks-with-relative-information-rate/</id>
    <title>Online learning for spiking neural networks with relative information rate</title>
    <updated>2023-12-11T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Spiking neural networks (SNNs) mimic biological neural networks more closely than feedforward neural networks (FNNs), and are pursued because of the promise of low-energy training and inference. While learning methods such as Hebbian algorithms and STDP (spiking-timing-dependent plasticity) exist, the learning theory of SNNs is poorly understood. In particular, little is known about how SNNs with memory (i.e. latent or hidden variables) can be trained effectively, making it difficult to build large SNNs that rival the performance of large FNNs. In this talk, we attack this problem with the information theory of time series. Using relative information rate, Amari’s em algorithm and stochastic approximation theory, we derive online learning algorithms for SNNs with memory. It turns out that STDP is a consequence of this algorithm, rather than its basis. This is joint work with Tenzin Chan, Chris Hillar and Sarah Marzen.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2023-12-11-online-learning-for-spiking-neural-networks-with-relative-information-rate/"/>
    <summary>Spiking neural networks (SNNs) mimic biological neural networks more closely than feedforward neural networks (FNNs), and are pursued because of the promise of low-energy training and inference. While learning methods such as Hebbian algorithms and STDP (spiking-timing-dependent plasticity) exist, the learning theory of SNNs is poorly understood. In particular, little is known about how SNNs with memory (i.e. latent or hidden variables) can be trained effectively, making it difficult to build large SNNs that rival the performance of large FNNs. In this talk, we attack this problem with the information theory of time series. Using relative information rate, Amari’s em algorithm and stochastic approximation theory, we derive online learning algorithms for SNNs with memory. It turns out that STDP is a consequence of this algorithm, rather than its basis. This is joint work with Tenzin Chan, Chris Hillar and Sarah Marzen.</summary>
    <published>2023-12-11T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2023-10-25-relative-information-and-the-dual-numbers/</id>
    <title>Relative Information and the Dual Numbers</title>
    <updated>2023-10-25T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Relative information (Kullback-Leibler divergence) is a fundamental concept in statistics, machine learning and information theory.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2023-10-25-relative-information-and-the-dual-numbers/"/>
    <summary>Relative information (Kullback-Leibler divergence) is a fundamental concept in statistics, machine learning and information theory.</summary>
    <published>2023-10-25T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2023-08-20-references-on-information-cohomology/</id>
    <title>References on information cohomology</title>
    <updated>2023-08-20T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Some references on the cohomological nature of various information theoretic concepts such as entropy and relative information.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2023-08-20-references-on-information-cohomology/"/>
    <summary>Some references on the cohomological nature of various information theoretic concepts such as entropy and relative information.</summary>
    <published>2023-08-20T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2023-06-26-all-you-need-is-relative-information/</id>
    <title>All you need is relative information</title>
    <updated>2023-06-26T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;SLT has taught me that relative information
(or Kullback-Leibler divergence) is all you need.
For instance, the level sets of relative information
give us state density functions, whose Fourier,
Laplace and Mellin transforms reveal different
facets of learning algorithms and their ability
to generalize. In this talk, I will outline two
ongoing projects involving relative information.
The first project explores the information theory
of time series, for the purpose of understanding
language models and reinforcement learning. Using
relative information rate, we derive stochastic
learning algorithms for spiking neural networks
with memory. The second project explores cohomological
generalizations of relative information, building on
recent work by Pierre Baudot and Daniel Bennequin
and by Tai-Danae Bradley. The hope is to uncover
new tools for studying non-statistical spaces,
by analyzing the level sets of generalized
relative information and their transforms.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2023-06-26-all-you-need-is-relative-information/"/>
    <summary>SLT has taught me that relative information
(or Kullback-Leibler divergence) is all you need.
For instance, the level sets of relative information
give us state density functions, whose Fourier,
Laplace and Mellin transforms reveal different
facets of learning algorithms and their ability
to generalize. In this talk, I will outline two
ongoing projects involving relative information.
The first project explores the information theory
of time series, for the purpose of understanding
language models and reinforcement learning. Using
relative information rate, we derive stochastic
learning algorithms for spiking neural networks
with memory. The second project explores cohomological
generalizations of relative information, building on
recent work by Pierre Baudot and Daniel Bennequin
and by Tai-Danae Bradley. The hope is to uncover
new tools for studying non-statistical spaces,
by analyzing the level sets of generalized
relative information and their transforms.</summary>
    <published>2023-06-26T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2023-04-01-relative-information-is-motivic/</id>
    <title>Relative information is motivic</title>
    <updated>2023-04-01T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;A (Hochschild) cohomological view of relative information must be motivic!&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2023-04-01-relative-information-is-motivic/"/>
    <summary>A (Hochschild) cohomological view of relative information must be motivic!</summary>
    <published>2023-04-01T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2022-05-28-likelihood-greed-and-temperature-in-sequence-learning/</id>
    <title>Likelihood, greed and temperature in sequence learning</title>
    <updated>2022-05-28T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Imagine we have a model &lt;span class="math notranslate nohighlight"&gt;\(D(w)\)&lt;/span&gt; of a dynamical system with states &lt;span class="math notranslate nohighlight"&gt;\(s \in S,\)&lt;/span&gt; that is parametrized by some weight &lt;span class="math notranslate nohighlight"&gt;\(w \in W\)&lt;/span&gt;. Each state &lt;span class="math notranslate nohighlight"&gt;\(s\)&lt;/span&gt; comes with a set &lt;span class="math notranslate nohighlight"&gt;\(N(s) \subset S\)&lt;/span&gt; of neighbors and an associated energy function &lt;span class="math notranslate nohighlight"&gt;\(E(s'|s,w) \in \mathbb{R}\)&lt;/span&gt; that assigns an energy to each neighbor &lt;span class="math notranslate nohighlight"&gt;\(s' \in N(s)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2022-05-28-likelihood-greed-and-temperature-in-sequence-learning/"/>
    <summary>Imagine we have a model D(w) of a dynamical system with states s \in S, that is parametrized by some weight w \in W. Each state s comes with a set N(s) \subset S of neighbors and an associated energy function E(s'|s,w) \in \mathbb{R} that assigns an energy to each neighbor s' \in N(s).</summary>
    <published>2022-05-28T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2022-05-08-parametric-typeclasses-aid-generalization-in-program-synthesis/</id>
    <title>Parametric typeclasses aid generalization in program synthesis</title>
    <updated>2022-01-22T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;We envision programming being done in top-down fashion. The human describes the goal (e.g. sorting), and the machine reduces it to smaller subgoals based on well-known heuristics (e.g. divide and conquer). The easier subgoals could even be fulfilled automatically. This top-down heuristics approach will be more amenable to machine learning. See my &lt;a class="reference internal" href="../posts/2021-04-22-proofs-as-programs-challenges-and-strategies-for-program-synthesis/"&gt;&lt;span class="std std-doc"&gt;Topos Institute talk&lt;/span&gt;&lt;/a&gt; for more info.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2022-05-08-parametric-typeclasses-aid-generalization-in-program-synthesis/"/>
    <summary>We envision programming being done in top-down fashion. The human describes the goal (e.g. sorting), and the machine reduces it to smaller subgoals based on well-known heuristics (e.g. divide and conquer). The easier subgoals could even be fulfilled automatically. This top-down heuristics approach will be more amenable to machine learning. See my Topos Institute talk for more info.</summary>
    <published>2022-01-22T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2022-01-22-information-topos-theory-motivation/</id>
    <title>Information topos theory - motivation</title>
    <updated>2022-01-22T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Relative information (also known as the Kullback-Leibler divergence) is an important fundamental concept in statistical learning and information theory.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2022-01-22-information-topos-theory-motivation/"/>
    <summary>Relative information (also known as the Kullback-Leibler divergence) is an important fundamental concept in statistical learning and information theory.</summary>
    <published>2022-01-22T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-09-09-all-you-need-is-relative-information/</id>
    <title>All you need is relative information</title>
    <updated>2021-09-09T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Relative information (relative entropy, KL divergence) and variational inference are powerful tools for deriving learning algorithms and their asymptotic properties, for both static systems and dynamic systems. The goal of this talk is to motivate a general online stochastic learning algorithm for stochastic processes with latent variables or memory, that provably converges under some regularity conditions. Please visit &lt;a class="reference external" href="https://bit.ly/3kmovql"&gt;https://bit.ly/3kmovql&lt;/a&gt; for details.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-09-09-all-you-need-is-relative-information/"/>
    <summary>Relative information (relative entropy, KL divergence) and variational inference are powerful tools for deriving learning algorithms and their asymptotic properties, for both static systems and dynamic systems. The goal of this talk is to motivate a general online stochastic learning algorithm for stochastic processes with latent variables or memory, that provably converges under some regularity conditions. Please visit https://bit.ly/3kmovql for details.</summary>
    <published>2021-09-09T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-06-05-spiking-neural-networks/</id>
    <title>Spiking neural networks</title>
    <updated>2021-06-05T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;In this post, we study a class of spiking network models based on continuous-time &lt;a class="reference internal" href="#2020-10-14-path-integrals-and-continuous-time-markov-chains/#what-is-a-continuous-time-markov-chain"&gt;&lt;span class="xref myst"&gt;Markov&lt;/span&gt;&lt;/a&gt; chains with &lt;a class="reference internal" href="#2021-03-22-relative-inference-with-mutable-processes/#what-is-a-mutable-process"&gt;&lt;span class="xref myst"&gt;mutable&lt;/span&gt;&lt;/a&gt;  variables.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-06-05-spiking-neural-networks/"/>
    <summary>In this post, we study a class of spiking network models based on continuous-time Markov chains with mutable  variables.</summary>
    <published>2021-06-05T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-06-01-convergence-of-biased-stochastic-approximation/</id>
    <title>Convergence of biased stochastic approximation</title>
    <updated>2021-06-01T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Using techniques from &lt;a class="reference internal" href="#2020-12-01-biased-stochastic-approximation/"&gt;&lt;span class="xref myst"&gt;biased&lt;/span&gt;&lt;/a&gt; stochastic approximation &lt;span id="id1"&gt;[&lt;a class="reference internal" href="../posts/2021-06-05-spiking-neural-networks/#id51" title="Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, 1944–1974. PMLR, 2019."&gt;KMMW19&lt;/a&gt;]&lt;/span&gt;, we prove under some regularity conditions the convergence of the online learning algorithm proposed &lt;a class="reference internal" href="#2021-03-23-biased-stochastic-approximation-with-mutable-processes/"&gt;&lt;span class="xref myst"&gt;previously&lt;/span&gt;&lt;/a&gt; for mutable Markov processes.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-06-01-convergence-of-biased-stochastic-approximation/"/>
    <summary>Using techniques from biased stochastic approximation karimi2019non, we prove under some regularity conditions the convergence of the online learning algorithm proposed previously for mutable Markov processes.</summary>
    <published>2021-06-01T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-05-10-path-integrals-and-the-dyson-formula/</id>
    <title>Path integrals and the Dyson formula</title>
    <updated>2021-05-10T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;One of the deepest results in quantum field theory, to me, is the Dyson formula &lt;span id="id1"&gt;[&lt;a class="reference internal" href="../posts/2021-05-10-path-integrals-and-the-dyson-formula/#id32" title="nLab. Dyson formula. https://ncatlab.org/nlab/show/Dyson+formula. Accessed: 2021."&gt;nLa&lt;/a&gt;]&lt;/span&gt;. It describes the solution to the differential equation&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-05-10-path-integrals-and-the-dyson-formula/"/>
    <summary>One of the deepest results in quantum field theory, to me, is the Dyson formula nlab2021dysonformula. It describes the solution to the differential equation</summary>
    <published>2021-05-10T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-04-22-proofs-as-programs-challenges-and-strategies-for-program-synthesis/</id>
    <title>Proofs as programs - challenges and strategies for program synthesis</title>
    <updated>2021-04-22T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;The Curry-Howard correspondence between proofs and programs suggests that we can exploit proof assistants for writing software. I will discuss the challenges behind a naïve execution of this idea, and some preliminary strategies for overcoming them. As an example, we will organize higher-order information in knowledge graphs using dependent type theory, and automate the answering of queries using a proof assistant. In another example, we will explore how decentralized proof assistants can enable mathematicians or programmers to work collaboratively on a theorem or application. If time permits, I will outline connections to canonical structures, reflection (ssreflect), transport, unification and universe management.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-04-22-proofs-as-programs-challenges-and-strategies-for-program-synthesis/"/>
    <summary>The Curry-Howard correspondence between proofs and programs suggests that we can exploit proof assistants for writing software. I will discuss the challenges behind a naïve execution of this idea, and some preliminary strategies for overcoming them. As an example, we will organize higher-order information in knowledge graphs using dependent type theory, and automate the answering of queries using a proof assistant. In another example, we will explore how decentralized proof assistants can enable mathematicians or programmers to work collaboratively on a theorem or application. If time permits, I will outline connections to canonical structures, reflection (ssreflect), transport, unification and universe management.</summary>
    <published>2021-04-22T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-03-23-biased-stochastic-approximation-with-mutable-processes/</id>
    <title>Biased stochastic approximation with mutable processes</title>
    <updated>2021-03-23T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;The goal of this post is to derive a general online learning recipe for training a &lt;a class="reference internal" href="#2021-03-22-relative-inference-with-mutable-processes/#what-is-a-mutable-process"&gt;&lt;span class="xref myst"&gt;mutable&lt;/span&gt;&lt;/a&gt; process &lt;span class="math notranslate nohighlight"&gt;\(\{Z_t,X_t\}\)&lt;/span&gt; to learn the true distribution &lt;span class="math notranslate nohighlight"&gt;\(Q_*(X)\)&lt;/span&gt; of a partially-observed Markov process &lt;span class="math notranslate nohighlight"&gt;\(\{X_t\}\)&lt;/span&gt;. The recipe returns a generative distribution &lt;span class="math notranslate nohighlight"&gt;\(P(Z,X)\)&lt;/span&gt; whose marginal &lt;span class="math notranslate nohighlight"&gt;\(P(X)\)&lt;/span&gt; approximates &lt;span class="math notranslate nohighlight"&gt;\(Q_*(X).\)&lt;/span&gt;&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-03-23-biased-stochastic-approximation-with-mutable-processes/"/>
    <summary>The goal of this post is to derive a general online learning recipe for training a mutable process \{Z_t,X_t\} to learn the true distribution Q_*(X) of a partially-observed Markov process \{X_t\}. The recipe returns a generative distribution P(Z,X) whose marginal P(X) approximates Q_*(X).</summary>
    <published>2021-03-23T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-03-22-relative-inference-with-mutable-processes/</id>
    <title>Relative inference with mutable processes</title>
    <updated>2021-03-22T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;We introduce a information-theoretic objective, which is a form of relative information between a discriminative model and a generative model, for learning processes using models with &lt;a class="reference internal" href="#2020-10-23-machine-learning-with-relative-information/#why-should-we-consider-mutable-variables-rather-than-latent-variables"&gt;&lt;span class="xref myst"&gt;mutable&lt;/span&gt;&lt;/a&gt; variables. This technique is known as &lt;a class="reference internal" href="#2020-10-23-machine-learning-with-relative-information/#why-do-we-need-a-better-name-for-variational-inference"&gt;&lt;span class="xref myst"&gt;relative inference&lt;/span&gt;&lt;/a&gt; (also called approximate inference, variational inference or variational Bayes). Such a technique is useful, for instance, for learning processes that contain latent variables.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-03-22-relative-inference-with-mutable-processes/"/>
    <summary>We introduce a information-theoretic objective, which is a form of relative information between a discriminative model and a generative model, for learning processes using models with mutable variables. This technique is known as relative inference (also called approximate inference, variational inference or variational Bayes). Such a technique is useful, for instance, for learning processes that contain latent variables.</summary>
    <published>2021-03-22T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2021-03-21-process-learning-with-relative-information/</id>
    <title>Process learning with relative information</title>
    <updated>2021-03-21T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Over the next few posts, we will derive a distributed learning algorithm for spiking neural networks with &lt;a class="reference internal" href="#2020-10-23-machine-learning-with-relative-information/#why-should-we-consider-mutable-variables-rather-than-latent-variables"&gt;&lt;span class="xref myst"&gt;mutable&lt;/span&gt;&lt;/a&gt; variables that minimizes some natural notion of relative information and provably converges over time. We will model these spiking neural networks with stochastic processes: both discrete-time and continuous-time processes, with or without mutable variables.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2021-03-21-process-learning-with-relative-information/"/>
    <summary>Over the next few posts, we will derive a distributed learning algorithm for spiking neural networks with mutable variables that minimizes some natural notion of relative information and provably converges over time. We will model these spiking neural networks with stochastic processes: both discrete-time and continuous-time processes, with or without mutable variables.</summary>
    <published>2021-03-21T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-12-01-biased-stochastic-approximation/</id>
    <title>Biased stochastic approximation</title>
    <updated>2020-12-01T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;We explore the convergence of continuous-time ordinary differential equations and their discrete-time analogs, such as stochastic approximation and gradient descent, through the lens of Lyapunov theory &lt;span id="id1"&gt;[&lt;a class="reference internal" href="../posts/2020-12-01-biased-stochastic-approximation/#id47" title="Léon Bottou and others. Online learning and stochastic approximations. On-line learning in neural networks, 17(9):142, 1998."&gt;B+98&lt;/a&gt;]&lt;/span&gt; &lt;span id="id2"&gt;[&lt;a class="reference internal" href="../posts/2020-12-01-biased-stochastic-approximation/#id44" title="Yingshen Li and Mark Rowland. Stochastic approximation theory (slides). http://yingzhenli.net/home/pdf/SA.pdf, 2015."&gt;LR15&lt;/a&gt;]&lt;/span&gt;. From this perspective, we will study biased stochastic approximation &lt;span id="id3"&gt;[&lt;a class="reference internal" href="../posts/2021-06-05-spiking-neural-networks/#id51" title="Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, 1944–1974. PMLR, 2019."&gt;KMMW19&lt;/a&gt;]&lt;/span&gt; where the expectation of the stochastic updates conditioned on the past (which we call the &lt;em&gt;conditional expectation&lt;/em&gt;) is not the same as the expectation of the stochastic updates under the stationary distribution (which we call the &lt;em&gt;total expectation&lt;/em&gt;).&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-12-01-biased-stochastic-approximation/"/>
    <summary>We explore the convergence of continuous-time ordinary differential equations and their discrete-time analogs, such as stochastic approximation and gradient descent, through the lens of Lyapunov theory bottou1998online li2015stochastic. From this perspective, we will study biased stochastic approximation karimi2019non where the expectation of the stochastic updates conditioned on the past (which we call the conditional expectation) is not the same as the expectation of the stochastic updates under the stationary distribution (which we call the total expectation).</summary>
    <published>2020-12-01T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-10-23-machine-learning-with-relative-information/</id>
    <title>Machine learning with relative information</title>
    <updated>2020-10-23T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;We will reframe some common machine learning paradigms, such as maximum likelihood, stochastic gradients, stochastic approximation and variational inference, in terms of relative information.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-10-23-machine-learning-with-relative-information/"/>
    <summary>We will reframe some common machine learning paradigms, such as maximum likelihood, stochastic gradients, stochastic approximation and variational inference, in terms of relative information.</summary>
    <published>2020-10-23T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-10-14-path-integrals-and-continuous-time-markov-chains/</id>
    <title>Path integrals and continuous-time Markov chains</title>
    <updated>2020-10-14T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;We give an introduction to continuous-time Markov chains, and define path measures for these objects.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-10-14-path-integrals-and-continuous-time-markov-chains/"/>
    <summary>We give an introduction to continuous-time Markov chains, and define path measures for these objects.</summary>
    <published>2020-10-14T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-10-07-motivic-relative-information/</id>
    <title>Motivic relative information</title>
    <updated>2020-10-07T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;So far, our definition of relative information studies the divergence between real-valued measures. In this post, we will explore motivic measures which take values more generally in some ring &lt;span class="math notranslate nohighlight"&gt;\(R\)&lt;/span&gt;, and have some fun applying motivic relative information to zeta functions.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-10-07-motivic-relative-information/"/>
    <summary>So far, our definition of relative information studies the divergence between real-valued measures. In this post, we will explore motivic measures which take values more generally in some ring R, and have some fun applying motivic relative information to zeta functions.</summary>
    <published>2020-10-07T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-10-05-zeta-functions-mellin-transforms-and-the-gelfand-leray-form/</id>
    <title>Zeta functions, Mellin transforms and the Gelfand-Leray form</title>
    <updated>2020-10-05T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;We outline the similarities between zeta functions appearing in number theory and in statistical learning.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-10-05-zeta-functions-mellin-transforms-and-the-gelfand-leray-form/"/>
    <summary>We outline the similarities between zeta functions appearing in number theory and in statistical learning.</summary>
    <published>2020-10-05T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-09-18-conditional-relative-information-and-its-axiomatizations/</id>
    <title>Conditional relative information and its axiomatizations</title>
    <updated>2020-09-18T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;In this post, we will study the conditional form of relative information. We will also look at how conditional relative information can be axiomatized and extended to non-real-valued measures.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-09-18-conditional-relative-information-and-its-axiomatizations/"/>
    <summary>In this post, we will study the conditional form of relative information. We will also look at how conditional relative information can be axiomatized and extended to non-real-valued measures.</summary>
    <published>2020-09-18T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-09-08-building-foundations-of-information-theory-on-relative-information/</id>
    <title>Building foundations of information theory on relative information</title>
    <updated>2020-09-08T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;The relative information &lt;span id="id1"&gt;[&lt;a class="reference internal" href="../posts/2022-01-22-information-topos-theory-motivation/#id66" title="JC Baez and T Fritz. A bayesian characterization of relative entropy. Theory and Applications of Categories, 29:422–456, 2014."&gt;BF14&lt;/a&gt;]&lt;/span&gt; (also known as relative entropy or Kullback-Leibler divergence) is an important object in information theory for measuring how far a probability measure &lt;span class="math notranslate nohighlight"&gt;\(Q\)&lt;/span&gt; is from another probability measure &lt;span class="math notranslate nohighlight"&gt;\(P.\)&lt;/span&gt; Here, &lt;span class="math notranslate nohighlight"&gt;\(Q\)&lt;/span&gt; is usually the true distribution of some real phenomenon, and &lt;span class="math notranslate nohighlight"&gt;\(P\)&lt;/span&gt; is some model distribution.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-09-08-building-foundations-of-information-theory-on-relative-information/"/>
    <summary>The relative information baez2014bayesian (also known as relative entropy or Kullback-Leibler divergence) is an important object in information theory for measuring how far a probability measure Q is from another probability measure P. Here, Q is usually the true distribution of some real phenomenon, and P is some model distribution.</summary>
    <published>2020-09-08T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-08-28-motivic-information-path-integrals-and-spiking-networks/</id>
    <title>Motivic information, path integrals and spiking networks</title>
    <updated>2020-08-28T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;I’m writing a series of posts that will explore the connections between these topics. Here is a rough outline of the series, which I will fill in slowly over time.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-08-28-motivic-information-path-integrals-and-spiking-networks/"/>
    <summary>I’m writing a series of posts that will explore the connections between these topics. Here is a rough outline of the series, which I will fill in slowly over time.</summary>
    <published>2020-08-28T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-08-07-processes-and-variety-maximization/</id>
    <title>Processes and variety maximization</title>
    <updated>2020-08-07T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;“It’s a theory about processes, about the sequences and causal relations among &lt;em&gt;things that happen&lt;/em&gt;, not the inherent properties of &lt;em&gt;things that are.&lt;/em&gt; The fundamental ingredient is what we call an “event.” Events are things that happen at a single place and time; at each event there’s some momentum, energy, charge or other various physical quantity that’s measurable. The event has relations with the rest of the universe, and that set of relations constitutes its “view” of the universe. Rather than describing an isolated system in terms of things that are measured from the outside, we’re taking the universe as constituted of relations among events. The idea is to try to reformulate physics in terms of these views from the inside, what it looks like from inside the universe.”&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-08-07-processes-and-variety-maximization/"/>
    <summary>“It’s a theory about processes, about the sequences and causal relations among things that happen, not the inherent properties of things that are. The fundamental ingredient is what we call an “event.” Events are things that happen at a single place and time; at each event there’s some momentum, energy, charge or other various physical quantity that’s measurable. The event has relations with the rest of the universe, and that set of relations constitutes its “view” of the universe. Rather than describing an isolated system in terms of things that are measured from the outside, we’re taking the universe as constituted of relations among events. The idea is to try to reformulate physics in terms of these views from the inside, what it looks like from inside the universe.”</summary>
    <published>2020-08-07T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-07-23-adjunctions/</id>
    <title>Adjunctions</title>
    <updated>2020-07-23T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Read about the Curry-Howard-Lambek correspondence. Some call it the holy trinity of Logic, Computation and Categories. Lambek adds the “objects as propositions” and “arrows as proofs” part to the mix. You may need to learn some basic category theory.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-07-23-adjunctions/"/>
    <summary>Read about the Curry-Howard-Lambek correspondence. Some call it the holy trinity of Logic, Computation and Categories. Lambek adds the “objects as propositions” and “arrows as proofs” part to the mix. You may need to learn some basic category theory.</summary>
    <published>2020-07-23T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-05-26-directed-spaces-and-types/</id>
    <title>Directed spaces and types</title>
    <updated>2020-05-26T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Joint work with Jin Xing Lim, Liang Ze Wong, Georgios Piliouras.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-05-26-directed-spaces-and-types/"/>
    <summary>Joint work with Jin Xing Lim, Liang Ze Wong, Georgios Piliouras.</summary>
    <published>2020-05-26T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2020-05-21-logical-frameworks/</id>
    <title>Logical frameworks</title>
    <updated>2020-05-21T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Logical frameworks are formal meta-languages for specifying different kinds of object theories (e.g. logical theories and type theories).&lt;/p&gt;
&lt;img alt="logical-framework-rules" src="https://shaoweilin.github.io/_images/logical-framework-rules.png" /&gt;&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2020-05-21-logical-frameworks/"/>
    <summary>Logical frameworks are formal meta-languages for specifying different kinds of object theories (e.g. logical theories and type theories).logical-framework-rules</summary>
    <published>2020-05-21T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2018-05-26-machine-reasoning-and-deep-spiking-networks/</id>
    <title>Machine reasoning and deep spiking networks</title>
    <updated>2018-05-26T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;This is a short presentation which I shared at the Visions of AI Futures workshop organized by AI Singapore. I talked about the goal of building next-generation machine intelligence, through neural and symbolic modules that work seamlessly together to accomplish intuitive reasoning, as well as efficient, effective neural chips for every device.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2018-05-26-machine-reasoning-and-deep-spiking-networks/"/>
    <summary>This is a short presentation which I shared at the Visions of AI Futures workshop organized by AI Singapore. I talked about the goal of building next-generation machine intelligence, through neural and symbolic modules that work seamlessly together to accomplish intuitive reasoning, as well as efficient, effective neural chips for every device.</summary>
    <published>2018-05-26T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2017-05-08-artificial-general-intelligence-for-the-internet-of-things/</id>
    <title>Artificial general intelligence for the internet of things</title>
    <updated>2017-05-08T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;What do we need to achieve artificial general intelligence? How do we distribute intelligence over the internet-of-things? We’ll dive deep into the heart of the matter, which is machine reasoning. Following recent advances in mathematical foundations and homotopy type theory, we conclude that the crux is to formally separate intents from implementations. We can teach neural networks to understand these intents and to use a divide-and-conquer method for compiling these intents into implementations. Our goal is to outline a distributed strategy for accomplishing this moonshot.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2017-05-08-artificial-general-intelligence-for-the-internet-of-things/"/>
    <summary>What do we need to achieve artificial general intelligence? How do we distribute intelligence over the internet-of-things? We’ll dive deep into the heart of the matter, which is machine reasoning. Following recent advances in mathematical foundations and homotopy type theory, we conclude that the crux is to formally separate intents from implementations. We can teach neural networks to understand these intents and to use a divide-and-conquer method for compiling these intents into implementations. Our goal is to outline a distributed strategy for accomplishing this moonshot.</summary>
    <published>2017-05-08T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2016-05-03-exercise-on-sparse-autoencoders/</id>
    <title>Exercise on sparse autoencoders</title>
    <updated>2016-05-03T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;This exercise is the first of several posts I am writing, for those who want a mathematical and hands-on introduction to deep neural networks.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2016-05-03-exercise-on-sparse-autoencoders/"/>
    <summary>This exercise is the first of several posts I am writing, for those who want a mathematical and hands-on introduction to deep neural networks.</summary>
    <published>2016-05-03T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2016-05-03-exercise-on-deep-neural-networks/</id>
    <title>Exercise on deep neural networks</title>
    <updated>2016-05-03T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Read the notes and complete the exercises for the section on “Building Deep Networks for Classification” in the &lt;a class="reference external" href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial"&gt;UFLDL Tutorial&lt;/a&gt;. Complete all the programming tasks in Python. No starter code will be given for these exercises, but you may refer to the given Matlab codes for hints if you are stuck&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2016-05-03-exercise-on-deep-neural-networks/"/>
    <summary>Read the notes and complete the exercises for the section on “Building Deep Networks for Classification” in the UFLDL Tutorial. Complete all the programming tasks in Python. No starter code will be given for these exercises, but you may refer to the given Matlab codes for hints if you are stuck</summary>
    <published>2016-05-03T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2016-02-04-hashing/</id>
    <title>Hashing</title>
    <updated>2016-02-04T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Hashing is a method for compressing information from a high dimensional space into a smaller space. Hashing is commonly used in computer science to help us with many tasks. For instance, if two documents are (randomly) hashed to the same code, it is very likely that they are exactly the same. Also, in computer vision, we sometimes hash images in a clever way to find similar or related images through their codes.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2016-02-04-hashing/"/>
    <summary>Hashing is a method for compressing information from a high dimensional space into a smaller space. Hashing is commonly used in computer science to help us with many tasks. For instance, if two documents are (randomly) hashed to the same code, it is very likely that they are exactly the same. Also, in computer vision, we sometimes hash images in a clever way to find similar or related images through their codes.</summary>
    <published>2016-02-04T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2014-08-13-statistics-and-machine-learning/</id>
    <title>Statistics and machine learning</title>
    <updated>2014-08-13T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Below are some introductory texts for probability, statistics, machine learning and statistical learning theory. They are sorted roughly according to difficulty, so you can find a book that is suitable for where you are and what you need.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2014-08-13-statistics-and-machine-learning/"/>
    <summary>Below are some introductory texts for probability, statistics, machine learning and statistical learning theory. They are sorted roughly according to difficulty, so you can find a book that is suitable for where you are and what you need.</summary>
    <published>2014-08-13T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2014-08-02-boltzmann-machines-and-hierarchical-models/</id>
    <title>Boltzmann machines and hierarchical models</title>
    <updated>2014-08-02T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;The &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"&gt;restricted Boltzmann machine&lt;/a&gt; (RBM) is a key statistical model used in &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Deep_learning"&gt;deep learning&lt;/a&gt;. They are special form of Boltzmann machines where the underlying graph is a bipartite graph. Personally, I am more interested in &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Boltzmann_machine"&gt;Boltzmann machines&lt;/a&gt; because they represent a class of discrete &lt;a class="reference external" href="http://www.cs.nyu.edu/~yann/research/ebm/"&gt;energy models&lt;/a&gt; where the energy is quadratic. The dynamics of the model bears a lot of resemblance to those of &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Hopfield_network"&gt;Hopfield networks&lt;/a&gt; and &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Ising_model"&gt;Ising models&lt;/a&gt;. As an aside, normal distributions are continuous energy models where the energy is quadratic and positive definite.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2014-08-02-boltzmann-machines-and-hierarchical-models/"/>
    <summary>The restricted Boltzmann machine (RBM) is a key statistical model used in deep learning. They are special form of Boltzmann machines where the underlying graph is a bipartite graph. Personally, I am more interested in Boltzmann machines because they represent a class of discrete energy models where the energy is quadratic. The dynamics of the model bears a lot of resemblance to those of Hopfield networks and Ising models. As an aside, normal distributions are continuous energy models where the energy is quadratic and positive definite.</summary>
    <published>2014-08-02T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://shaoweilin.github.io/posts/2012-07-13-studying-model-asymptotics-with-singular-learning-theory/</id>
    <title>Studying model asymptotics with singular learning theory</title>
    <updated>2012-07-13T00:00:00+00:00</updated>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Singular statistical models occur frequently in machine learning and computational biology. An important prob-lem in the learning theory of singular models is determin-ing their asymptotic behavior for massive data sets. In this talk, we give a brief introduction to Sumio Watanabe’s Singular Learning Theory, as outlined in his book “Algebraic Geometry and Statistical Learning Theory.” We will also explore the rich algebraic geometry and combinatorics that arise from studying the asymptotics of Bayesian integrals.&lt;/p&gt;
&lt;/p&gt;
</content>
    <link href="https://shaoweilin.github.io/posts/2012-07-13-studying-model-asymptotics-with-singular-learning-theory/"/>
    <summary>Singular statistical models occur frequently in machine learning and computational biology. An important prob-lem in the learning theory of singular models is determin-ing their asymptotic behavior for massive data sets. In this talk, we give a brief introduction to Sumio Watanabe’s Singular Learning Theory, as outlined in his book “Algebraic Geometry and Statistical Learning Theory.” We will also explore the rich algebraic geometry and combinatorics that arise from studying the asymptotics of Bayesian integrals.</summary>
    <published>2012-07-13T00:00:00+00:00</published>
  </entry>
</feed>
